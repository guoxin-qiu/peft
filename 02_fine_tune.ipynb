{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c7ba1e7-b18d-40d6-9be4-f5c44be13119",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers 微调训练入门"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8befbc0d-011d-4ec5-af5d-0703ed07c44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46f4e186-6546-4077-8a9d-24227421a506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abbb0e319914b9c9d00683991cee18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be99540308e0466cb522ccbf665b2c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Yelp评论完整星级数据集是通过随机选取每个1到5星评论的130,000个训练样本和10,000个测试样本构建的。总共有650,000个训练样本和50,000个测试样本。\n",
    "train_full_ds = Dataset.from_parquet(\"r_data/yelp_review_full/train-00000-of-00001.parquet\")\n",
    "test_full_ds = Dataset.from_parquet(\"r_data/yelp_review_full/test-00000-of-00001.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12d6f6bb-b64b-4dd1-b909-6372a442c346",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    'train': train_full_ds,\n",
    "    'test': test_full_ds\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa827756-818d-45ea-83a4-85ecf52f5bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 650000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "891b690e-8543-41a0-a88e-1834047b69a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 2,\n",
       " 'text': \"As far as Starbucks go, this is a pretty nice one.  The baristas are friendly and while I was here, a lot of regulars must have come in, because they bantered away with almost everyone.  The bathroom was clean and well maintained and the trash wasn't overflowing in the canisters around the store.  The pastries looked fresh, but I didn't partake.  The noise level was also at a nice working level - not too loud, music just barely audible.\\\\n\\\\nI do wish there was more seating.  It is nice that this location has a counter at the end of the bar for sole workers, but it doesn't replace more tables.  I'm sure this isn't as much of a problem in the summer when there's the space outside.\\\\n\\\\nThere was a treat receipt promo going on, but the barista didn't tell me about it, which I found odd.  Usually when they have promos like that going on, they ask everyone if they want their receipt to come back later in the day to claim whatever the offer is.  Today it was one of their new pastries for $1, I know in the summer they do $2 grande iced drinks with that morning's receipt.\\\\n\\\\nOverall, nice working or socializing environment.  Very friendly and inviting.  It's what I've come to expect from Starbucks, so points for consistency.\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42905ffe-12be-4731-8f9c-40c04914efaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9adbf23-9948-4a06-8208-d54b3c3ef687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64687cca-6ab4-4ced-b149-c128566ef9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>My husband and I first tried this food truck while at Urban Cookies, and now .... we log on at least weekly to see where we can follow them to. Ya, it can be a little pricey, but soooo worth it. And the luncharitas are amazing!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>I discovered this place while waiting for the 64 bus after leaving the Manor movie theatre.  The place was small and the sign in front is barely noticeable and I fully expected to see Dozens bakery when I looked through the window, but I was pleasantly surprised to see waffle irons at work.  I have been dying to find a place that serves fresh hot waffles for a long time now. My boyfriend and I tried the fancified versions of the waffle at first, mine topped with whipped cream and fresh strawberries and his with a chocolate-hazelnut spread (I can't remember if it was Nutella or some other brand) and sliced bananas.  Regardless of the small \\\"dining\\\" area we were fully satisfied with our waffles.  They were warm and fluffy and light so you get a fulfilling snack without it being too heavy.  Perfect snack after the movie while we waited for the bus.  We stopped in on a different occasion after dinner on a cold rainy day to grab a few of the plain sugar waffles to go.  These were perfectly portable and delicious.  I think I prefer them this way, without all of the extra toppings.  \\n\\nOne other thing that I like about this place is that they accept cards, which is very convenient for people like me, who never carry any cash.  \\n\\nIt's too bad that I don't live in Squirrel Hill anymore.  Otherwise, I'd stop by more often.  They need to advertise their shop a bit more because I had no idea that they were there.  I even missed the sign in front.  I love the idea of the waffles being made right in front of the glass window though.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>I'm still angry at my best buddy, who lives/lived in Vegas, for not taking me here sooner.  HOLY COW.  SO DELICIOUS.  I cry when I think of the red velvet pancakes.  I cry even more when thinking of the kalua pig omelette.  \\n\\nWe went for brunch on Friday at around 11:30 AM.  There were a group of 6 before us but other than that, not much of a wait.  We got our seats within 10 minutes.  Love that they had Hawaii-an food selection.  i ordered the kalua pig omelette while one bff got a crepe and the other bff (who i'm mad at), got the red velvet panackes. \\n\\nThe crepe, was just a crepe.  But the pancakes were warm and delicious and so much like a red velvet cupcake.  A perfect accompaniment to my slightly salty kalua pig.\\n\\nThe kalua pig was perfectly salted and made me miss home just a bit.  The portion was generous; if I were going home after brunch, I'd save some to eat for later.  But I wasn't so I went ahead and ate most of it.  My crepe-bff ate what I couldn't finish.\\n\\nI can't wait to come back to Vegas for this place!  Slots/gambling/clubbing, I'm not such a huge fan.  But this place, is worth flying back for.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>Koi Las Vegas has some major hype going on around it!  Kim Kardashian, Kevin Spacey, David Spade......if you flip on the Extra channel in the new Planet Hollywood Hotel, it seems like all the celebrities have something nice to say about Koi.  The original Koi is in Los Angeles, and frequently gets celebrities hopping in and out of that restaurant.  \\n\\nFood-wise, this place is no match for Matsuhisa in Los Angeles, but it is a nice place to have dinner if you're on the Strip.  Koi Las Vegas is on the Mezzanine level in the Planet Hollywood Hotel &amp; Casino.  Three jolly buddhas are positioned in front of the restaurant, and then you walk around the circular lounge, and into the dimly lit restaurant flanked by modern grey/green booths and glossy brown tables. It's very dark in the restaurant, with just the sushi/sashimi chefs lit up under recessed lighting near the sushi bar.   Lots of businessmen at this restaurant....those darn companies pick up the pricey tab when dinner is all over.  Wish I could have a company expense!  Some people were there for dates.  The dress code is dressy casual, but they will let you in if you look semi-decent (ie no flip flops or T-shirts, but everything else, they'll let you in.)\\n\\nLike many other fusion Japanese restaurants, Koi Las Vegas has a signature crispy rice spicy tuna appetizer.  4 pieces of spicy tuna with a hint of jalepeno slice, over a crunchy, chewy fried piece of rice.  Expensive, yes, at $15 for only four pieces, but it was pretty good.  The crunchy, chewy rice was warm and complemented the cold spicy tuna on top.  The slice of jalepeno gave the dish a nice kick in the end.  Lots of customers were getting this dish!\\n\\nThe salmon carpaccio with black truffle was not bad, but I think I\\\"ve had better.  The salmon was ultra thin, and I expected the carpaccio to be chunks of salmon together in a black truffle oil sauce, but to me, this dish was underwhelming.  The truffle was chopped up and placed on the salmon slices, and it was okay.  I think this dish could be better if there was more originality to the presentation.  For the price, this dish could have been better.\\n\\nThe miso crusted black cod was delicious, and they gave a hearty portion to the dish.  It was laced with some baby carrots, and some asparagus on the side.  The cod was light, flaky and the miso sauce had just the right amount of flavor to compliment the dish.  If you come to Koi, get this dish, because you'll be happy with your choice.  Light, yet flavorful, this dish is appealing to all seafood lovers.  Original, no---because you can find this dish at just about any other Asian fusion restaurant out there, and it tastes just as good at any other restaurant (ie think Roy's).  \\n\\nThe chilean sea bass was very very light, as it was steamed with ginger and scallion.  It is placed on a bed of shitake mushrooms and you can really enjoy the flavor of the sea bass.  Very fresh.  \\n\\nFor dessert, do not get the molten chocolate cake, because it is nothing like Roy's, and you do not get the hot, lava flowing chocolate from the inside of the cake.  \\n\\nI only gave Koi Las Vegas 3 stars because it is not a stand out restaurant, and the dishes were not spectacular like at other places.  At Roy's, there's more originality, and you feel like you're getting a good deal for the price you pay.  Here at Koi, they rely a bit too much on the celebrity hype, so the food suffers.  The place is nothing like the Los Angeles Koi---it's not cozy, and you can't really appreciate the decor since it so dark inside.  There's a small view of the Las Vegas Strip, but most tables don't have a view because only a small portion of the tables are facing that direction.\\n\\nExpensive meal, but not really worth it in my opinion.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>Friendly service. We were the only customers so service was good. Food was good and cheap. left happy!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2 star</td>\n",
       "      <td>I have to admit that I'm really surprised Mister G's has a current rating of something like 4.5.  I've been on a few occasions and had them deliver once and I just don't like their pizza.  It's not good.\\n\\nI ordered a pizza and wings special that they hung on my door once and I gotta tell you, man:  The pizza tasted like it had been frozen and bought at a store.  The wings tasted good, they TASTED good, but the texture was odd.  It was like eating an approximation of what human food is supposed to be like as imagined by an alien species.\\n\\nThe ambience on the inside when you want to eat in is really just not condusive to a great time.  It looks like it would be better if it were only a bar, but the fact that it's identified with as being a pizzeria makes me like \\\"meh,\\\" about the overall effect of their decor.\\n\\nI remember when this place used to be Roma's or whatever it was before.  It's more or less similar to what it had been previously.\\n\\nNo offense to Mister G. himself.  It seems like he could really kick my ass if he wanted to.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>Had dinner here out on the patio. Location was excellent had a very good view of the performers while eating. Server was polite and kind even though we were obviously tourists. The food was excellent and priced similar to other restaurants nearby.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>Apr\\u00e8s une visite \\u00e0 l'insectarium, tout un aprem de d\\u00e9tente au milieu des plantes. J'y suis allez pendant les mosaicsculpture c'\\u00e9tait vraiment magnifique, malgr\\u00e9 la foule de gens pr\\u00e9sents. Mais on s'est eloign\\u00e9 \\u00e0 l'autre bout du parc sous les arbres et l\\u00e0 on a vraiment pu profiter m\\u00eame si il y avait un lointain bruit de voiture.\\n\\nLes diff\\u00e9rents jardins sont vraiment beaux, malgr\\u00e9 que je soit un peu d\\u00e9\\u00e7u par le jardin japonais.\\n\\nCe que j'ai vraiment appr\\u00e9ci\\u00e9 c'est leur collection de Bonzai qui est vraiment vari\\u00e9 et impressionnante.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2 star</td>\n",
       "      <td>Points for being quick, relatively inexpensive food that's open late, but the tuna sandwich was very lackluster. :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2 star</td>\n",
       "      <td>I haven't eaten at this location, so although I'm sure it's the exact same food as the other L3B's in the area, I won't comment on the food quality here.\\n\\nI don't know what it is with their beers, it's something that I can't quite verbalize.  I tried several different beers: the blanche, the blonde, the IPA (which was a new item on the menu) and a couple of others, including a seasonal.  They all had this... funk about them, except for the IPA. Like a hint of clove and foot.  My wife noticed it immediately as well.  In fact, even though she doesn't care for IPA's, the IPA was still her favorite of the group (because it was sans funk).\\n\\nIf you're not limited to the immediate area, I'd branch out and try a different place for a good micro brew.  Or honestly, either stick to the IPA (or maybe the stout, which I didn't try) or find another place with a decent selection that's not made in-house.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba3d6da-ae28-4695-ac14-97c3da8e8ce3",
   "metadata": {},
   "source": [
    "## 预处理数据\n",
    "\n",
    "下载数据集到本地后，使用 Tokenizer 来处理文本，对于长度不等的输入数据，可以使用填充（padding）和截断（truncation）策略来处理。\n",
    "\n",
    "Datasets 的 `map` 方法，支持一次性在整个数据集上应用预处理函数。\n",
    "\n",
    "下面使用填充到最大长度的策略，处理整个数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eeed8c2-5642-4ecc-82e5-4fa1134fc9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4f58491c3f4ea099b0f66ff27f05a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/650000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5297ab1174405190403dccc728e3dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/models/bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "431dcf9a-a08b-44b2-918b-27961a894eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d80d03f3c94ccc988b465afd612ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/5 shards):   0%|          | 0/650000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d9d30a6ce04a96a9221eef91fb6ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets.save_to_disk(\"r_data/yelp_review_full/tokenized_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b221154e-352a-4f49-b856-8e4673b4d665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 650000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c308712-437e-4663-8110-db007b446c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "tokenized_datasets = load_from_disk(\"r_data/yelp_review_full/tokenized_datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190ca0dd-f28d-44c4-bbd0-d5a58fcd981a",
   "metadata": {},
   "source": [
    "### 数据抽样\n",
    "\n",
    "使用 N 个数据样本，在 BERT 上演示小规模训练（基于 Pytorch Trainer）\n",
    "\n",
    "`shuffle()`函数会随机重新排列列的值。如果您希望对用于洗牌数据集的算法有更多控制，可以在此函数中指定generator参数来使用不同的numpy.random.Generator。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8792ff80-3802-45e7-9a5f-425a626c0166",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=66).select(range(100))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=66).select(range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbffa77-0206-40b9-975c-246dc3767623",
   "metadata": {},
   "source": [
    "## 微调训练配置\n",
    "\n",
    "### 加载 BERT 模型\n",
    "\n",
    "警告通知我们正在丢弃一些权重（`vocab_transform` 和 `vocab_layer_norm` 层），并随机初始化其他一些权重（`pre_classifier` 和 `classifier` 层）。在微调模型情况下是绝对正常的，因为我们正在删除用于预训练模型的掩码语言建模任务的头部，并用一个新的头部替换它，对于这个新头部，我们没有预训练的权重，所以库会警告我们在用它进行推理之前应该对这个模型进行微调，而这正是我们要做的事情。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "358fdcf5-9505-473c-aa3b-a3bd60a308c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /data/models/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"/data/models/bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9ab690-4f3b-40b2-880f-42ef1ccc5d55",
   "metadata": {},
   "source": [
    "### 训练超参数（TrainingArguments）\n",
    "\n",
    "完整配置参数与默认值：\n",
    "- https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.TrainingArguments\n",
    "- https://hf-mirror.com/docs/transformers/training\n",
    "\n",
    "源代码定义：https://github.com/huggingface/transformers/blob/v4.36.1/src/transformers/training_args.py#L161\n",
    "\n",
    "**最重要配置：模型权重保存路径(output_dir)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af729bc1-5c20-4ebd-b52f-de9757c7c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "model_dir = \"/data/models/bert-base-cased-finetune-yelp-100\"\n",
    "\n",
    "# logging_steps 默认值为500，根据我们的训练数据和步长，将其设置为50\n",
    "training_args = TrainingArguments(output_dir=model_dir,\n",
    "                                  per_device_train_batch_size=16,\n",
    "                                  num_train_epochs=5,\n",
    "                                  logging_steps=50,\n",
    "                                  save_total_limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f6f357f-4da5-4acb-bd5d-f7ea3094f9da",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m\n",
       "TrainingArguments(\n",
       "    output_dir: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    overwrite_output_dir: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    do_train: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    do_eval: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    do_predict: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = \u001b[33m'no'\u001b[39m,\n",
       "    prediction_loss_only: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    per_device_train_batch_size: int = \u001b[32m8\u001b[39m,\n",
       "    per_device_eval_batch_size: int = \u001b[32m8\u001b[39m,\n",
       "    per_gpu_train_batch_size: Optional[int] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    per_gpu_eval_batch_size: Optional[int] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    gradient_accumulation_steps: int = \u001b[32m1\u001b[39m,\n",
       "    eval_accumulation_steps: Optional[int] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    eval_delay: Optional[float] = \u001b[32m0\u001b[39m,\n",
       "    torch_empty_cache_steps: Optional[int] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    learning_rate: float = \u001b[32m5e-05\u001b[39m,\n",
       "    weight_decay: float = \u001b[32m0.0\u001b[39m,\n",
       "    adam_beta1: float = \u001b[32m0.9\u001b[39m,\n",
       "    adam_beta2: float = \u001b[32m0.999\u001b[39m,\n",
       "    adam_epsilon: float = \u001b[32m1e-08\u001b[39m,\n",
       "    max_grad_norm: float = \u001b[32m1.0\u001b[39m,\n",
       "    num_train_epochs: float = \u001b[32m3.0\u001b[39m,\n",
       "    max_steps: int = -\u001b[32m1\u001b[39m,\n",
       "    lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = \u001b[33m'linear'\u001b[39m,\n",
       "    lr_scheduler_kwargs: Union[dict[str, Any], str, NoneType] = <factory>,\n",
       "    warmup_ratio: float = \u001b[32m0.0\u001b[39m,\n",
       "    warmup_steps: int = \u001b[32m0\u001b[39m,\n",
       "    log_level: str = \u001b[33m'passive'\u001b[39m,\n",
       "    log_level_replica: str = \u001b[33m'warning'\u001b[39m,\n",
       "    log_on_each_node: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
       "    logging_dir: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = \u001b[33m'steps'\u001b[39m,\n",
       "    logging_first_step: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    logging_steps: float = \u001b[32m500\u001b[39m,\n",
       "    logging_nan_inf_filter: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
       "    save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = \u001b[33m'steps'\u001b[39m,\n",
       "    save_steps: float = \u001b[32m500\u001b[39m,\n",
       "    save_total_limit: Optional[int] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    save_safetensors: Optional[bool] = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
       "    save_on_each_node: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    save_only_model: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    restore_callback_states_from_checkpoint: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    no_cuda: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    use_cpu: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    use_mps_device: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    seed: int = \u001b[32m42\u001b[39m,\n",
       "    data_seed: Optional[int] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    jit_mode_eval: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    use_ipex: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    bf16: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    fp16: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    fp16_opt_level: str = \u001b[33m'O1'\u001b[39m,\n",
       "    half_precision_backend: str = \u001b[33m'auto'\u001b[39m,\n",
       "    bf16_full_eval: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    fp16_full_eval: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    tf32: Optional[bool] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    local_rank: int = -\u001b[32m1\u001b[39m,\n",
       "    ddp_backend: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    tpu_num_cores: Optional[int] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    tpu_metrics_debug: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    debug: Union[str, list[transformers.debug_utils.DebugOption]] = \u001b[33m''\u001b[39m,\n",
       "    dataloader_drop_last: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    eval_steps: Optional[float] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    dataloader_num_workers: int = \u001b[32m0\u001b[39m,\n",
       "    dataloader_prefetch_factor: Optional[int] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    past_index: int = -\u001b[32m1\u001b[39m,\n",
       "    run_name: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    disable_tqdm: Optional[bool] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    remove_unused_columns: Optional[bool] = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
       "    label_names: Optional[list[str]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    load_best_model_at_end: Optional[bool] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    metric_for_best_model: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    greater_is_better: Optional[bool] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    ignore_data_skip: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    fsdp: Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = \u001b[33m''\u001b[39m,\n",
       "    fsdp_min_num_params: int = \u001b[32m0\u001b[39m,\n",
       "    fsdp_config: Union[dict[str, Any], str, NoneType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    fsdp_transformer_layer_cls_to_wrap: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    accelerator_config: Union[str, dict, NoneType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    deepspeed: Union[str, dict, NoneType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    label_smoothing_factor: float = \u001b[32m0.0\u001b[39m,\n",
       "    optim: Union[transformers.training_args.OptimizerNames, str] = \u001b[33m'adamw_torch'\u001b[39m,\n",
       "    optim_args: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    adafactor: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    group_by_length: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    length_column_name: Optional[str] = \u001b[33m'length'\u001b[39m,\n",
       "    report_to: Union[NoneType, str, list[str]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    ddp_find_unused_parameters: Optional[bool] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    ddp_bucket_cap_mb: Optional[int] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    ddp_broadcast_buffers: Optional[bool] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    dataloader_pin_memory: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
       "    dataloader_persistent_workers: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    skip_memory_metrics: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
       "    use_legacy_prediction_loop: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    push_to_hub: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    resume_from_checkpoint: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    hub_model_id: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = \u001b[33m'every_save'\u001b[39m,\n",
       "    hub_token: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    hub_private_repo: Optional[bool] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    hub_always_push: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    hub_revision: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    gradient_checkpointing: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    gradient_checkpointing_kwargs: Union[dict[str, Any], str, NoneType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    include_inputs_for_metrics: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    include_for_metrics: list[str] = <factory>,\n",
       "    eval_do_concat_batches: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
       "    fp16_backend: str = \u001b[33m'auto'\u001b[39m,\n",
       "    push_to_hub_model_id: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    push_to_hub_organization: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    push_to_hub_token: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    mp_parameters: str = \u001b[33m''\u001b[39m,\n",
       "    auto_find_batch_size: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    full_determinism: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    torchdynamo: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    ray_scope: Optional[str] = \u001b[33m'last'\u001b[39m,\n",
       "    ddp_timeout: int = \u001b[32m1800\u001b[39m,\n",
       "    torch_compile: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    torch_compile_backend: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    torch_compile_mode: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    include_tokens_per_second: Optional[bool] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    include_num_input_tokens_seen: Optional[bool] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    neftune_noise_alpha: Optional[float] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    optim_target_modules: Union[NoneType, str, list[str]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    batch_eval_metrics: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    eval_on_start: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    use_liger_kernel: Optional[bool] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    liger_kernel_config: Optional[dict[str, bool]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    eval_use_gather_object: Optional[bool] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    average_tokens_across_devices: Optional[bool] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       ") -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[31mDocstring:\u001b[39m     \n",
       "TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop\n",
       "itself**.\n",
       "\n",
       "Using [`HfArgumentParser`] we can turn this class into\n",
       "[argparse](https://docs.python.org/3/library/argparse#module-argparse) arguments that can be specified on the\n",
       "command line.\n",
       "\n",
       "Parameters:\n",
       "    output_dir (`str`, *optional*, defaults to `\"trainer_output\"`):\n",
       "        The output directory where the model predictions and checkpoints will be written.\n",
       "    overwrite_output_dir (`bool`, *optional*, defaults to `False`):\n",
       "        If `True`, overwrite the content of the output directory. Use this to continue training if `output_dir`\n",
       "        points to a checkpoint directory.\n",
       "    do_train (`bool`, *optional*, defaults to `False`):\n",
       "        Whether to run training or not. This argument is not directly used by [`Trainer`], it's intended to be used\n",
       "        by your training/evaluation scripts instead. See the [example\n",
       "        scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
       "    do_eval (`bool`, *optional*):\n",
       "        Whether to run evaluation on the validation set or not. Will be set to `True` if `eval_strategy` is\n",
       "        different from `\"no\"`. This argument is not directly used by [`Trainer`], it's intended to be used by your\n",
       "        training/evaluation scripts instead. See the [example\n",
       "        scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
       "    do_predict (`bool`, *optional*, defaults to `False`):\n",
       "        Whether to run predictions on the test set or not. This argument is not directly used by [`Trainer`], it's\n",
       "        intended to be used by your training/evaluation scripts instead. See the [example\n",
       "        scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
       "    eval_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"no\"`):\n",
       "        The evaluation strategy to adopt during training. Possible values are:\n",
       "\n",
       "            - `\"no\"`: No evaluation is done during training.\n",
       "            - `\"steps\"`: Evaluation is done (and logged) every `eval_steps`.\n",
       "            - `\"epoch\"`: Evaluation is done at the end of each epoch.\n",
       "\n",
       "    prediction_loss_only (`bool`, *optional*, defaults to `False`):\n",
       "        When performing evaluation and generating predictions, only returns the loss.\n",
       "    per_device_train_batch_size (`int`, *optional*, defaults to 8):\n",
       "        The batch size per device accelerator core/CPU for training.\n",
       "    per_device_eval_batch_size (`int`, *optional*, defaults to 8):\n",
       "        The batch size per device accelerator core/CPU for evaluation.\n",
       "    gradient_accumulation_steps (`int`, *optional*, defaults to 1):\n",
       "        Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
       "\n",
       "        <Tip warning={true}>\n",
       "\n",
       "        When using gradient accumulation, one step is counted as one step with backward pass. Therefore, logging,\n",
       "        evaluation, save will be conducted every `gradient_accumulation_steps * xxx_step` training examples.\n",
       "\n",
       "        </Tip>\n",
       "\n",
       "    eval_accumulation_steps (`int`, *optional*):\n",
       "        Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU. If\n",
       "        left unset, the whole predictions are accumulated on the device accelerator before being moved to the CPU (faster but\n",
       "        requires more memory).\n",
       "    eval_delay (`float`, *optional*):\n",
       "        Number of epochs or steps to wait for before the first evaluation can be performed, depending on the\n",
       "        eval_strategy.\n",
       "    torch_empty_cache_steps (`int`, *optional*):\n",
       "        Number of steps to wait before calling `torch.<device>.empty_cache()`. If left unset or set to None, cache will not be emptied.\n",
       "\n",
       "        <Tip>\n",
       "\n",
       "        This can help avoid CUDA out-of-memory errors by lowering peak VRAM usage at a cost of about [10% slower performance](https://github.com/huggingface/transformers/issues/31372).\n",
       "\n",
       "        </Tip>\n",
       "\n",
       "    learning_rate (`float`, *optional*, defaults to 5e-5):\n",
       "        The initial learning rate for [`AdamW`] optimizer.\n",
       "    weight_decay (`float`, *optional*, defaults to 0):\n",
       "        The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in [`AdamW`]\n",
       "        optimizer.\n",
       "    adam_beta1 (`float`, *optional*, defaults to 0.9):\n",
       "        The beta1 hyperparameter for the [`AdamW`] optimizer.\n",
       "    adam_beta2 (`float`, *optional*, defaults to 0.999):\n",
       "        The beta2 hyperparameter for the [`AdamW`] optimizer.\n",
       "    adam_epsilon (`float`, *optional*, defaults to 1e-8):\n",
       "        The epsilon hyperparameter for the [`AdamW`] optimizer.\n",
       "    max_grad_norm (`float`, *optional*, defaults to 1.0):\n",
       "        Maximum gradient norm (for gradient clipping).\n",
       "    num_train_epochs(`float`, *optional*, defaults to 3.0):\n",
       "        Total number of training epochs to perform (if not an integer, will perform the decimal part percents of\n",
       "        the last epoch before stopping training).\n",
       "    max_steps (`int`, *optional*, defaults to -1):\n",
       "        If set to a positive number, the total number of training steps to perform. Overrides `num_train_epochs`.\n",
       "        For a finite dataset, training is reiterated through the dataset (if all data is exhausted) until\n",
       "        `max_steps` is reached.\n",
       "    lr_scheduler_type (`str` or [`SchedulerType`], *optional*, defaults to `\"linear\"`):\n",
       "        The scheduler type to use. See the documentation of [`SchedulerType`] for all possible values.\n",
       "    lr_scheduler_kwargs ('dict', *optional*, defaults to {}):\n",
       "        The extra arguments for the lr_scheduler. See the documentation of each scheduler for possible values.\n",
       "    warmup_ratio (`float`, *optional*, defaults to 0.0):\n",
       "        Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.\n",
       "    warmup_steps (`int`, *optional*, defaults to 0):\n",
       "        Number of steps used for a linear warmup from 0 to `learning_rate`. Overrides any effect of `warmup_ratio`.\n",
       "    log_level (`str`, *optional*, defaults to `passive`):\n",
       "        Logger log level to use on the main process. Possible choices are the log levels as strings: 'debug',\n",
       "        'info', 'warning', 'error' and 'critical', plus a 'passive' level which doesn't set anything and keeps the\n",
       "        current log level for the Transformers library (which will be `\"warning\"` by default).\n",
       "    log_level_replica (`str`, *optional*, defaults to `\"warning\"`):\n",
       "        Logger log level to use on replicas. Same choices as `log_level`\"\n",
       "    log_on_each_node (`bool`, *optional*, defaults to `True`):\n",
       "        In multinode distributed training, whether to log using `log_level` once per node, or only on the main\n",
       "        node.\n",
       "    logging_dir (`str`, *optional*):\n",
       "        [TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\n",
       "        *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\n",
       "    logging_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"steps\"`):\n",
       "        The logging strategy to adopt during training. Possible values are:\n",
       "\n",
       "            - `\"no\"`: No logging is done during training.\n",
       "            - `\"epoch\"`: Logging is done at the end of each epoch.\n",
       "            - `\"steps\"`: Logging is done every `logging_steps`.\n",
       "\n",
       "    logging_first_step (`bool`, *optional*, defaults to `False`):\n",
       "        Whether to log the first `global_step` or not.\n",
       "    logging_steps (`int` or `float`, *optional*, defaults to 500):\n",
       "        Number of update steps between two logs if `logging_strategy=\"steps\"`. Should be an integer or a float in\n",
       "        range `[0,1)`. If smaller than 1, will be interpreted as ratio of total training steps.\n",
       "    logging_nan_inf_filter (`bool`, *optional*, defaults to `True`):\n",
       "        Whether to filter `nan` and `inf` losses for logging. If set to `True` the loss of every step that is `nan`\n",
       "        or `inf` is filtered and the average loss of the current logging window is taken instead.\n",
       "\n",
       "        <Tip>\n",
       "\n",
       "        `logging_nan_inf_filter` only influences the logging of loss values, it does not change the behavior the\n",
       "        gradient is computed or applied to the model.\n",
       "\n",
       "        </Tip>\n",
       "\n",
       "    save_strategy (`str` or [`~trainer_utils.SaveStrategy`], *optional*, defaults to `\"steps\"`):\n",
       "        The checkpoint save strategy to adopt during training. Possible values are:\n",
       "\n",
       "            - `\"no\"`: No save is done during training.\n",
       "            - `\"epoch\"`: Save is done at the end of each epoch.\n",
       "            - `\"steps\"`: Save is done every `save_steps`.\n",
       "            - `\"best\"`: Save is done whenever a new `best_metric` is achieved.\n",
       "\n",
       "            If `\"epoch\"` or `\"steps\"` is chosen, saving will also be performed at the\n",
       "            very end of training, always.\n",
       "    save_steps (`int` or `float`, *optional*, defaults to 500):\n",
       "        Number of updates steps before two checkpoint saves if `save_strategy=\"steps\"`. Should be an integer or a\n",
       "        float in range `[0,1)`. If smaller than 1, will be interpreted as ratio of total training steps.\n",
       "    save_total_limit (`int`, *optional*):\n",
       "        If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in\n",
       "        `output_dir`. When `load_best_model_at_end` is enabled, the \"best\" checkpoint according to\n",
       "        `metric_for_best_model` will always be retained in addition to the most recent ones. For example, for\n",
       "        `save_total_limit=5` and `load_best_model_at_end`, the four last checkpoints will always be retained\n",
       "        alongside the best model. When `save_total_limit=1` and `load_best_model_at_end`, it is possible that two\n",
       "        checkpoints are saved: the last one and the best one (if they are different).\n",
       "    save_safetensors (`bool`, *optional*, defaults to `True`):\n",
       "        Use [safetensors](https://huggingface.co/docs/safetensors) saving and loading for state dicts instead of\n",
       "        default `torch.load` and `torch.save`.\n",
       "    save_on_each_node (`bool`, *optional*, defaults to `False`):\n",
       "        When doing multi-node distributed training, whether to save models and checkpoints on each node, or only on\n",
       "        the main one.\n",
       "\n",
       "        This should not be activated when the different nodes use the same storage as the files will be saved with\n",
       "        the same names for each node.\n",
       "    save_only_model (`bool`, *optional*, defaults to `False`):\n",
       "        When checkpointing, whether to only save the model, or also the optimizer, scheduler & rng state.\n",
       "        Note that when this is true, you won't be able to resume training from checkpoint.\n",
       "        This enables you to save storage by not storing the optimizer, scheduler & rng state.\n",
       "        You can only load the model using `from_pretrained` with this option set to `True`.\n",
       "    restore_callback_states_from_checkpoint (`bool`, *optional*, defaults to `False`):\n",
       "        Whether to restore the callback states from the checkpoint. If `True`, will override\n",
       "        callbacks passed to the `Trainer` if they exist in the checkpoint.\"\n",
       "    use_cpu (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to use cpu. If set to False, we will use cuda or mps device if available.\n",
       "    seed (`int`, *optional*, defaults to 42):\n",
       "        Random seed that will be set at the beginning of training. To ensure reproducibility across runs, use the\n",
       "        [`~Trainer.model_init`] function to instantiate the model if it has some randomly initialized parameters.\n",
       "    data_seed (`int`, *optional*):\n",
       "        Random seed to be used with data samplers. If not set, random generators for data sampling will use the\n",
       "        same seed as `seed`. This can be used to ensure reproducibility of data sampling, independent of the model\n",
       "        seed.\n",
       "    jit_mode_eval (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to use PyTorch jit trace for inference.\n",
       "    use_ipex (`bool`, *optional*, defaults to `False`):\n",
       "        Use Intel extension for PyTorch when it is available. [IPEX\n",
       "        installation](https://github.com/intel/intel-extension-for-pytorch).\n",
       "    bf16 (`bool`, *optional*, defaults to `False`):\n",
       "        Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training. Requires Ampere or higher\n",
       "        NVIDIA architecture or Intel XPU or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change.\n",
       "    fp16 (`bool`, *optional*, defaults to `False`):\n",
       "        Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n",
       "    fp16_opt_level (`str`, *optional*, defaults to 'O1'):\n",
       "        For `fp16` training, Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details on\n",
       "        the [Apex documentation](https://nvidia.github.io/apex/amp).\n",
       "    fp16_backend (`str`, *optional*, defaults to `\"auto\"`):\n",
       "        This argument is deprecated. Use `half_precision_backend` instead.\n",
       "    half_precision_backend (`str`, *optional*, defaults to `\"auto\"`):\n",
       "        The backend to use for mixed precision training. Must be one of `\"auto\", \"apex\", \"cpu_amp\"`. `\"auto\"` will\n",
       "        use CPU/CUDA AMP or APEX depending on the PyTorch version detected, while the other choices will force the\n",
       "        requested backend.\n",
       "    bf16_full_eval (`bool`, *optional*, defaults to `False`):\n",
       "        Whether to use full bfloat16 evaluation instead of 32-bit. This will be faster and save memory but can harm\n",
       "        metric values. This is an experimental API and it may change.\n",
       "    fp16_full_eval (`bool`, *optional*, defaults to `False`):\n",
       "        Whether to use full float16 evaluation instead of 32-bit. This will be faster and save memory but can harm\n",
       "        metric values.\n",
       "    tf32 (`bool`, *optional*):\n",
       "        Whether to enable the TF32 mode, available in Ampere and newer GPU architectures. The default value depends\n",
       "        on PyTorch's version default of `torch.backends.cuda.matmul.allow_tf32`. For more details please refer to\n",
       "        the [TF32](https://huggingface.co/docs/transformers/perf_train_gpu_one#tf32) documentation. This is an\n",
       "        experimental API and it may change.\n",
       "    local_rank (`int`, *optional*, defaults to -1):\n",
       "        Rank of the process during distributed training.\n",
       "    ddp_backend (`str`, *optional*):\n",
       "        The backend to use for distributed training. Must be one of `\"nccl\"`, `\"mpi\"`, `\"ccl\"`, `\"gloo\"`, `\"hccl\"`.\n",
       "    tpu_num_cores (`int`, *optional*):\n",
       "        When training on TPU, the number of TPU cores (automatically passed by launcher script).\n",
       "    dataloader_drop_last (`bool`, *optional*, defaults to `False`):\n",
       "        Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)\n",
       "        or not.\n",
       "    eval_steps (`int` or `float`, *optional*):\n",
       "        Number of update steps between two evaluations if `eval_strategy=\"steps\"`. Will default to the same\n",
       "        value as `logging_steps` if not set. Should be an integer or a float in range `[0,1)`. If smaller than 1,\n",
       "        will be interpreted as ratio of total training steps.\n",
       "    dataloader_num_workers (`int`, *optional*, defaults to 0):\n",
       "        Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the\n",
       "        main process.\n",
       "    past_index (`int`, *optional*, defaults to -1):\n",
       "        Some models like [TransformerXL](../model_doc/transformerxl) or [XLNet](../model_doc/xlnet) can make use of\n",
       "        the past hidden states for their predictions. If this argument is set to a positive int, the `Trainer` will\n",
       "        use the corresponding output (usually index 2) as the past state and feed it to the model at the next\n",
       "        training step under the keyword argument `mems`.\n",
       "    run_name (`str`, *optional*, defaults to `output_dir`):\n",
       "        A descriptor for the run. Typically used for [wandb](https://www.wandb.com/),\n",
       "        [mlflow](https://www.mlflow.org/), [comet](https://www.comet.com/site) and [swanlab](https://swanlab.cn)\n",
       "        logging. If not specified, will be the same as `output_dir`.\n",
       "    disable_tqdm (`bool`, *optional*):\n",
       "        Whether or not to disable the tqdm progress bars and table of metrics produced by\n",
       "        [`~notebook.NotebookTrainingTracker`] in Jupyter Notebooks. Will default to `True` if the logging level is\n",
       "        set to warn or lower (default), `False` otherwise.\n",
       "    remove_unused_columns (`bool`, *optional*, defaults to `True`):\n",
       "        Whether or not to automatically remove the columns unused by the model forward method.\n",
       "    label_names (`list[str]`, *optional*):\n",
       "        The list of keys in your dictionary of inputs that correspond to the labels.\n",
       "\n",
       "        Will eventually default to the list of argument names accepted by the model that contain the word \"label\",\n",
       "        except if the model used is one of the `XxxForQuestionAnswering` in which case it will also include the\n",
       "        `[\"start_positions\", \"end_positions\"]` keys.\n",
       "    load_best_model_at_end (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to load the best model found during training at the end of training. When this option is\n",
       "        enabled, the best checkpoint will always be saved. See\n",
       "        [`save_total_limit`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_total_limit)\n",
       "        for more.\n",
       "\n",
       "        <Tip>\n",
       "\n",
       "        When set to `True`, the parameters `save_strategy` needs to be the same as `eval_strategy`, and in\n",
       "        the case it is \"steps\", `save_steps` must be a round multiple of `eval_steps`.\n",
       "\n",
       "        </Tip>\n",
       "\n",
       "    metric_for_best_model (`str`, *optional*):\n",
       "        Use in conjunction with `load_best_model_at_end` to specify the metric to use to compare two different\n",
       "        models. Must be the name of a metric returned by the evaluation with or without the prefix `\"eval_\"`.\n",
       "\n",
       "        If not specified, this will default to `\"loss\"` when either `load_best_model_at_end == True`\n",
       "        or `lr_scheduler_type == SchedulerType.REDUCE_ON_PLATEAU` (to use the evaluation loss).\n",
       "\n",
       "        If you set this value, `greater_is_better` will default to `True` unless the name ends with \"loss\".\n",
       "        Don't forget to set it to `False` if your metric is better when lower.\n",
       "    greater_is_better (`bool`, *optional*):\n",
       "        Use in conjunction with `load_best_model_at_end` and `metric_for_best_model` to specify if better models\n",
       "        should have a greater metric or not. Will default to:\n",
       "\n",
       "        - `True` if `metric_for_best_model` is set to a value that doesn't end in `\"loss\"`.\n",
       "        - `False` if `metric_for_best_model` is not set, or set to a value that ends in `\"loss\"`.\n",
       "    ignore_data_skip (`bool`, *optional*, defaults to `False`):\n",
       "        When resuming training, whether or not to skip the epochs and batches to get the data loading at the same\n",
       "        stage as in the previous training. If set to `True`, the training will begin faster (as that skipping step\n",
       "        can take a long time) but will not yield the same results as the interrupted training would have.\n",
       "    fsdp (`bool`, `str` or list of [`~trainer_utils.FSDPOption`], *optional*, defaults to `''`):\n",
       "        Use PyTorch Distributed Parallel Training (in distributed training only).\n",
       "\n",
       "        A list of options along the following:\n",
       "\n",
       "        - `\"full_shard\"`: Shard parameters, gradients and optimizer states.\n",
       "        - `\"shard_grad_op\"`: Shard optimizer states and gradients.\n",
       "        - `\"hybrid_shard\"`: Apply `FULL_SHARD` within a node, and replicate parameters across nodes.\n",
       "        - `\"hybrid_shard_zero2\"`: Apply `SHARD_GRAD_OP` within a node, and replicate parameters across nodes.\n",
       "        - `\"offload\"`: Offload parameters and gradients to CPUs (only compatible with `\"full_shard\"` and\n",
       "          `\"shard_grad_op\"`).\n",
       "        - `\"auto_wrap\"`: Automatically recursively wrap layers with FSDP using `default_auto_wrap_policy`.\n",
       "    fsdp_config (`str` or `dict`, *optional*):\n",
       "        Config to be used with fsdp (Pytorch Distributed Parallel Training). The value is either a location of\n",
       "        fsdp json config file (e.g., `fsdp_config.json`) or an already loaded json file as `dict`.\n",
       "\n",
       "        A List of config and its options:\n",
       "            - min_num_params (`int`, *optional*, defaults to `0`):\n",
       "                FSDP's minimum number of parameters for Default Auto Wrapping. (useful only when `fsdp` field is\n",
       "                passed).\n",
       "            - transformer_layer_cls_to_wrap (`list[str]`, *optional*):\n",
       "                List of transformer layer class names (case-sensitive) to wrap, e.g, `BertLayer`, `GPTJBlock`,\n",
       "                `T5Block` .... (useful only when `fsdp` flag is passed).\n",
       "            - backward_prefetch (`str`, *optional*)\n",
       "                FSDP's backward prefetch mode. Controls when to prefetch next set of parameters (useful only when\n",
       "                `fsdp` field is passed).\n",
       "\n",
       "                A list of options along the following:\n",
       "\n",
       "                - `\"backward_pre\"` : Prefetches the next set of parameters before the current set of parameter's\n",
       "                  gradient\n",
       "                    computation.\n",
       "                - `\"backward_post\"` : This prefetches the next set of parameters after the current set of\n",
       "                  parameter’s\n",
       "                    gradient computation.\n",
       "            - forward_prefetch (`bool`, *optional*, defaults to `False`)\n",
       "                FSDP's forward prefetch mode (useful only when `fsdp` field is passed).\n",
       "                 If `\"True\"`, then FSDP explicitly prefetches the next upcoming all-gather while executing in the\n",
       "                 forward pass.\n",
       "            - limit_all_gathers (`bool`, *optional*, defaults to `False`)\n",
       "                FSDP's limit_all_gathers (useful only when `fsdp` field is passed).\n",
       "                 If `\"True\"`, FSDP explicitly synchronizes the CPU thread to prevent too many in-flight\n",
       "                 all-gathers.\n",
       "            - use_orig_params (`bool`, *optional*, defaults to `True`)\n",
       "                If `\"True\"`, allows non-uniform `requires_grad` during init, which means support for interspersed\n",
       "                frozen and trainable parameters. Useful in cases such as parameter-efficient fine-tuning. Please\n",
       "                refer this\n",
       "                [blog](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019\n",
       "            - sync_module_states (`bool`, *optional*, defaults to `True`)\n",
       "                If `\"True\"`, each individually wrapped FSDP unit will broadcast module parameters from rank 0 to\n",
       "                ensure they are the same across all ranks after initialization\n",
       "            - cpu_ram_efficient_loading (`bool`, *optional*, defaults to `False`)\n",
       "                If `\"True\"`, only the first process loads the pretrained model checkpoint while all other processes\n",
       "                have empty weights.  When this setting as `\"True\"`, `sync_module_states` also must to be `\"True\"`,\n",
       "                otherwise all the processes except the main process would have random weights leading to unexpected\n",
       "                behaviour during training.\n",
       "            - activation_checkpointing (`bool`, *optional*, defaults to `False`):\n",
       "                If `\"True\"`, activation checkpointing is a technique to reduce memory usage by clearing activations of\n",
       "                certain layers and recomputing them during a backward pass. Effectively, this trades extra\n",
       "                computation time for reduced memory usage.\n",
       "            - xla (`bool`, *optional*, defaults to `False`):\n",
       "                Whether to use PyTorch/XLA Fully Sharded Data Parallel Training. This is an experimental feature\n",
       "                and its API may evolve in the future.\n",
       "            - xla_fsdp_settings (`dict`, *optional*)\n",
       "                The value is a dictionary which stores the XLA FSDP wrapping parameters.\n",
       "\n",
       "                For a complete list of options, please see [here](\n",
       "                https://github.com/pytorch/xla/blob/master/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py).\n",
       "            - xla_fsdp_grad_ckpt (`bool`, *optional*, defaults to `False`):\n",
       "                Will use gradient checkpointing over each nested XLA FSDP wrapped layer. This setting can only be\n",
       "                used when the xla flag is set to true, and an auto wrapping policy is specified through\n",
       "                fsdp_min_num_params or fsdp_transformer_layer_cls_to_wrap.\n",
       "    deepspeed (`str` or `dict`, *optional*):\n",
       "        Use [Deepspeed](https://github.com/deepspeedai/DeepSpeed). This is an experimental feature and its API may\n",
       "        evolve in the future. The value is either the location of DeepSpeed json config file (e.g.,\n",
       "        `ds_config.json`) or an already loaded json file as a `dict`\"\n",
       "\n",
       "        <Tip warning={true}>\n",
       "            If enabling any Zero-init, make sure that your model is not initialized until\n",
       "            *after* initializing the `TrainingArguments`, else it will not be applied.\n",
       "        </Tip>\n",
       "\n",
       "    accelerator_config (`str`, `dict`, or `AcceleratorConfig`, *optional*):\n",
       "        Config to be used with the internal `Accelerator` implementation. The value is either a location of\n",
       "        accelerator json config file (e.g., `accelerator_config.json`), an already loaded json file as `dict`,\n",
       "        or an instance of [`~trainer_pt_utils.AcceleratorConfig`].\n",
       "\n",
       "        A list of config and its options:\n",
       "            - split_batches (`bool`, *optional*, defaults to `False`):\n",
       "                Whether or not the accelerator should split the batches yielded by the dataloaders across the devices. If\n",
       "                `True` the actual batch size used will be the same on any kind of distributed processes, but it must be a\n",
       "                round multiple of the `num_processes` you are using. If `False`, actual batch size used will be the one set\n",
       "                in your script multiplied by the number of processes.\n",
       "            - dispatch_batches (`bool`, *optional*):\n",
       "                If set to `True`, the dataloader prepared by the Accelerator is only iterated through on the main process\n",
       "                and then the batches are split and broadcast to each process. Will default to `True` for `DataLoader` whose\n",
       "                underlying dataset is an `IterableDataset`, `False` otherwise.\n",
       "            - even_batches (`bool`, *optional*, defaults to `True`):\n",
       "                If set to `True`, in cases where the total batch size across all processes does not exactly divide the\n",
       "                dataset, samples at the start of the dataset will be duplicated so the batch can be divided equally among\n",
       "                all workers.\n",
       "            - use_seedable_sampler (`bool`, *optional*, defaults to `True`):\n",
       "                Whether or not use a fully seedable random sampler ([`accelerate.data_loader.SeedableRandomSampler`]). Ensures\n",
       "                training results are fully reproducible using a different sampling technique. While seed-to-seed results\n",
       "                may differ, on average the differences are negligible when using multiple different seeds to compare. Should\n",
       "                also be ran with [`~utils.set_seed`] for the best results.\n",
       "            - use_configured_state (`bool`, *optional*, defaults to `False`):\n",
       "                Whether or not to use a pre-configured `AcceleratorState` or `PartialState` defined before calling `TrainingArguments`.\n",
       "                If `True`, an `Accelerator` or `PartialState` must be initialized. Note that by doing so, this could lead to issues\n",
       "                with hyperparameter tuning.\n",
       "\n",
       "    label_smoothing_factor (`float`, *optional*, defaults to 0.0):\n",
       "        The label smoothing factor to use. Zero means no label smoothing, otherwise the underlying onehot-encoded\n",
       "        labels are changed from 0s and 1s to `label_smoothing_factor/num_labels` and `1 - label_smoothing_factor +\n",
       "        label_smoothing_factor/num_labels` respectively.\n",
       "    debug (`str` or list of [`~debug_utils.DebugOption`], *optional*, defaults to `\"\"`):\n",
       "        Enable one or more debug features. This is an experimental feature.\n",
       "\n",
       "        Possible options are:\n",
       "\n",
       "        - `\"underflow_overflow\"`: detects overflow in model's input/outputs and reports the last frames that led to\n",
       "          the event\n",
       "        - `\"tpu_metrics_debug\"`: print debug metrics on TPU\n",
       "\n",
       "        The options should be separated by whitespaces.\n",
       "    optim (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `\"adamw_torch\"`):\n",
       "        The optimizer to use, such as \"adamw_torch\", \"adamw_torch_fused\", \"adamw_apex_fused\", \"adamw_anyprecision\",\n",
       "        \"adafactor\". See `OptimizerNames` in [training_args.py](https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py)\n",
       "        for a full list of optimizers.\n",
       "    optim_args (`str`, *optional*):\n",
       "        Optional arguments that are supplied to optimizers such as AnyPrecisionAdamW, AdEMAMix, and GaLore.\n",
       "    group_by_length (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to group together samples of roughly the same length in the training dataset (to minimize\n",
       "        padding applied and be more efficient). Only useful if applying dynamic padding.\n",
       "    length_column_name (`str`, *optional*, defaults to `\"length\"`):\n",
       "        Column name for precomputed lengths. If the column exists, grouping by length will use these values rather\n",
       "        than computing them on train startup. Ignored unless `group_by_length` is `True` and the dataset is an\n",
       "        instance of `Dataset`.\n",
       "    report_to (`str` or `list[str]`, *optional*, defaults to `\"all\"`):\n",
       "        The list of integrations to report the results and logs to. Supported platforms are `\"azure_ml\"`,\n",
       "        `\"clearml\"`, `\"codecarbon\"`, `\"comet_ml\"`, `\"dagshub\"`, `\"dvclive\"`, `\"flyte\"`, `\"mlflow\"`, `\"neptune\"`,\n",
       "        `\"swanlab\"`, `\"tensorboard\"`, and `\"wandb\"`. Use `\"all\"` to report to all integrations installed, `\"none\"`\n",
       "        for no integrations.\n",
       "    ddp_find_unused_parameters (`bool`, *optional*):\n",
       "        When using distributed training, the value of the flag `find_unused_parameters` passed to\n",
       "        `DistributedDataParallel`. Will default to `False` if gradient checkpointing is used, `True` otherwise.\n",
       "    ddp_bucket_cap_mb (`int`, *optional*):\n",
       "        When using distributed training, the value of the flag `bucket_cap_mb` passed to `DistributedDataParallel`.\n",
       "    ddp_broadcast_buffers (`bool`, *optional*):\n",
       "        When using distributed training, the value of the flag `broadcast_buffers` passed to\n",
       "        `DistributedDataParallel`. Will default to `False` if gradient checkpointing is used, `True` otherwise.\n",
       "    dataloader_pin_memory (`bool`, *optional*, defaults to `True`):\n",
       "        Whether you want to pin memory in data loaders or not. Will default to `True`.\n",
       "    dataloader_persistent_workers (`bool`, *optional*, defaults to `False`):\n",
       "        If True, the data loader will not shut down the worker processes after a dataset has been consumed once.\n",
       "        This allows to maintain the workers Dataset instances alive. Can potentially speed up training, but will\n",
       "        increase RAM usage. Will default to `False`.\n",
       "    dataloader_prefetch_factor (`int`, *optional*):\n",
       "        Number of batches loaded in advance by each worker.\n",
       "        2 means there will be a total of 2 * num_workers batches prefetched across all workers.\n",
       "    skip_memory_metrics (`bool`, *optional*, defaults to `True`):\n",
       "        Whether to skip adding of memory profiler reports to metrics. This is skipped by default because it slows\n",
       "        down the training and evaluation speed.\n",
       "    push_to_hub (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to push the model to the Hub every time the model is saved. If this is activated,\n",
       "        `output_dir` will begin a git directory synced with the repo (determined by `hub_model_id`) and the content\n",
       "        will be pushed each time a save is triggered (depending on your `save_strategy`). Calling\n",
       "        [`~Trainer.save_model`] will also trigger a push.\n",
       "\n",
       "        <Tip warning={true}>\n",
       "\n",
       "        If `output_dir` exists, it needs to be a local clone of the repository to which the [`Trainer`] will be\n",
       "        pushed.\n",
       "\n",
       "        </Tip>\n",
       "\n",
       "    resume_from_checkpoint (`str`, *optional*):\n",
       "        The path to a folder with a valid checkpoint for your model. This argument is not directly used by\n",
       "        [`Trainer`], it's intended to be used by your training/evaluation scripts instead. See the [example\n",
       "        scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
       "    hub_model_id (`str`, *optional*):\n",
       "        The name of the repository to keep in sync with the local *output_dir*. It can be a simple model ID in\n",
       "        which case the model will be pushed in your namespace. Otherwise it should be the whole repository name,\n",
       "        for instance `\"user_name/model\"`, which allows you to push to an organization you are a member of with\n",
       "        `\"organization_name/model\"`. Will default to `user_name/output_dir_name` with *output_dir_name* being the\n",
       "        name of `output_dir`.\n",
       "\n",
       "        Will default to the name of `output_dir`.\n",
       "    hub_strategy (`str` or [`~trainer_utils.HubStrategy`], *optional*, defaults to `\"every_save\"`):\n",
       "        Defines the scope of what is pushed to the Hub and when. Possible values are:\n",
       "\n",
       "        - `\"end\"`: push the model, its configuration, the processing class e.g. tokenizer (if passed along to the [`Trainer`]) and a\n",
       "          draft of a model card when the [`~Trainer.save_model`] method is called.\n",
       "        - `\"every_save\"`: push the model, its configuration, the processing class e.g. tokenizer (if passed along to the [`Trainer`]) and\n",
       "          a draft of a model card each time there is a model save. The pushes are asynchronous to not block\n",
       "          training, and in case the save are very frequent, a new push is only attempted if the previous one is\n",
       "          finished. A last push is made with the final model at the end of training.\n",
       "        - `\"checkpoint\"`: like `\"every_save\"` but the latest checkpoint is also pushed in a subfolder named\n",
       "          last-checkpoint, allowing you to resume training easily with\n",
       "          `trainer.train(resume_from_checkpoint=\"last-checkpoint\")`.\n",
       "        - `\"all_checkpoints\"`: like `\"checkpoint\"` but all checkpoints are pushed like they appear in the output\n",
       "          folder (so you will get one checkpoint folder per folder in your final repository)\n",
       "\n",
       "    hub_token (`str`, *optional*):\n",
       "        The token to use to push the model to the Hub. Will default to the token in the cache folder obtained with\n",
       "        `huggingface-cli login`.\n",
       "    hub_private_repo (`bool`, *optional*):\n",
       "        Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.\n",
       "    hub_always_push (`bool`, *optional*, defaults to `False`):\n",
       "        Unless this is `True`, the `Trainer` will skip pushing a checkpoint when the previous push is not finished.\n",
       "    hub_revision (`str`, *optional*):\n",
       "        The revision to use when pushing to the Hub. Can be a branch name, a tag, or a commit hash.\n",
       "    gradient_checkpointing (`bool`, *optional*, defaults to `False`):\n",
       "        If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
       "    gradient_checkpointing_kwargs (`dict`, *optional*, defaults to `None`):\n",
       "        Key word arguments to be passed to the `gradient_checkpointing_enable` method.\n",
       "    include_inputs_for_metrics (`bool`, *optional*, defaults to `False`):\n",
       "        This argument is deprecated. Use `include_for_metrics` instead, e.g, `include_for_metrics = [\"inputs\"]`.\n",
       "    include_for_metrics (`list[str]`, *optional*, defaults to `[]`):\n",
       "        Include additional data in the `compute_metrics` function if needed for metrics computation.\n",
       "        Possible options to add to `include_for_metrics` list:\n",
       "        - `\"inputs\"`: Input data passed to the model, intended for calculating input dependent metrics.\n",
       "        - `\"loss\"`: Loss values computed during evaluation, intended for calculating loss dependent metrics.\n",
       "    eval_do_concat_batches (`bool`, *optional*, defaults to `True`):\n",
       "        Whether to recursively concat inputs/losses/labels/predictions across batches. If `False`,\n",
       "        will instead store them as lists, with each batch kept separate.\n",
       "    auto_find_batch_size (`bool`, *optional*, defaults to `False`)\n",
       "        Whether to find a batch size that will fit into memory automatically through exponential decay, avoiding\n",
       "        CUDA Out-of-Memory errors. Requires accelerate to be installed (`pip install accelerate`)\n",
       "    full_determinism (`bool`, *optional*, defaults to `False`)\n",
       "        If `True`, [`enable_full_determinism`] is called instead of [`set_seed`] to ensure reproducible results in\n",
       "        distributed training. Important: this will negatively impact the performance, so only use it for debugging.\n",
       "    torchdynamo (`str`, *optional*):\n",
       "        If set, the backend compiler for TorchDynamo. Possible choices are `\"eager\"`, `\"aot_eager\"`, `\"inductor\"`,\n",
       "        `\"nvfuser\"`, `\"aot_nvfuser\"`, `\"aot_cudagraphs\"`, `\"ofi\"`, `\"fx2trt\"`, `\"onnxrt\"` and `\"ipex\"`.\n",
       "    ray_scope (`str`, *optional*, defaults to `\"last\"`):\n",
       "        The scope to use when doing hyperparameter search with Ray. By default, `\"last\"` will be used. Ray will\n",
       "        then use the last checkpoint of all trials, compare those, and select the best one. However, other options\n",
       "        are also available. See the [Ray documentation](\n",
       "        https://docs.ray.io/en/latest/tune/api_docs/analysis.html#ray.tune.ExperimentAnalysis.get_best_trial) for\n",
       "        more options.\n",
       "    ddp_timeout (`int`, *optional*, defaults to 1800):\n",
       "        The timeout for `torch.distributed.init_process_group` calls, used to avoid GPU socket timeouts when\n",
       "        performing slow operations in distributed runnings. Please refer the [PyTorch documentation]\n",
       "        (https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group) for more\n",
       "        information.\n",
       "    use_mps_device (`bool`, *optional*, defaults to `False`):\n",
       "        This argument is deprecated.`mps` device will be used if it is available similar to `cuda` device.\n",
       "    torch_compile (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to compile the model using PyTorch 2.0\n",
       "        [`torch.compile`](https://pytorch.org/get-started/pytorch-2.0/).\n",
       "\n",
       "        This will use the best defaults for the [`torch.compile`\n",
       "        API](https://pytorch.org/docs/stable/generated/torch.compile.html?highlight=torch+compile#torch.compile).\n",
       "        You can customize the defaults with the argument `torch_compile_backend` and `torch_compile_mode` but we\n",
       "        don't guarantee any of them will work as the support is progressively rolled in in PyTorch.\n",
       "\n",
       "        This flag and the whole compile API is experimental and subject to change in future releases.\n",
       "    torch_compile_backend (`str`, *optional*):\n",
       "        The backend to use in `torch.compile`. If set to any value, `torch_compile` will be set to `True`.\n",
       "\n",
       "        Refer to the PyTorch doc for possible values and note that they may change across PyTorch versions.\n",
       "\n",
       "        This flag is experimental and subject to change in future releases.\n",
       "    torch_compile_mode (`str`, *optional*):\n",
       "        The mode to use in `torch.compile`. If set to any value, `torch_compile` will be set to `True`.\n",
       "\n",
       "        Refer to the PyTorch doc for possible values and note that they may change across PyTorch versions.\n",
       "\n",
       "        This flag is experimental and subject to change in future releases.\n",
       "    include_tokens_per_second (`bool`, *optional*):\n",
       "        Whether or not to compute the number of tokens per second per device for training speed metrics.\n",
       "\n",
       "        This will iterate over the entire training dataloader once beforehand,\n",
       "\n",
       "        and will slow down the entire process.\n",
       "\n",
       "    include_num_input_tokens_seen (`bool`, *optional*):\n",
       "        Whether or not to track the number of input tokens seen throughout training.\n",
       "\n",
       "        May be slower in distributed training as gather operations must be called.\n",
       "\n",
       "    neftune_noise_alpha (`Optional[float]`):\n",
       "        If not `None`, this will activate NEFTune noise embeddings. This can drastically improve model performance\n",
       "        for instruction fine-tuning. Check out the [original paper](https://huggingface.co/papers/2310.05914) and the\n",
       "        [original code](https://github.com/neelsjain/NEFTune). Support transformers `PreTrainedModel` and also\n",
       "        `PeftModel` from peft. The original paper used values in the range [5.0, 15.0].\n",
       "    optim_target_modules (`Union[str, list[str]]`, *optional*):\n",
       "        The target modules to optimize, i.e. the module names that you would like to train.\n",
       "        Currently used for the GaLore algorithm (https://huggingface.co/papers/2403.03507) and APOLLO algorithm (https://huggingface.co/papers/2412.05270).\n",
       "        See GaLore implementation (https://github.com/jiaweizzhao/GaLore) and APOLLO implementation (https://github.com/zhuhanqing/APOLLO) for more details.\n",
       "        You need to make sure to pass a valid GaLore or APOLLO optimizer, e.g., one of: \"apollo_adamw\", \"galore_adamw\", \"galore_adamw_8bit\", \"galore_adafactor\" and make sure that the target modules are `nn.Linear` modules only.\n",
       "\n",
       "    batch_eval_metrics (`Optional[bool]`, defaults to `False`):\n",
       "        If set to `True`, evaluation will call compute_metrics at the end of each batch to accumulate statistics\n",
       "        rather than saving all eval logits in memory. When set to `True`, you must pass a compute_metrics function\n",
       "        that takes a boolean argument `compute_result`, which when passed `True`, will trigger the final global\n",
       "        summary statistics from the batch-level summary statistics you've accumulated over the evaluation set.\n",
       "\n",
       "    eval_on_start (`bool`, *optional*, defaults to `False`):\n",
       "        Whether to perform a evaluation step (sanity check) before the training to ensure the validation steps works correctly.\n",
       "\n",
       "    eval_use_gather_object (`bool`, *optional*, defaults to `False`):\n",
       "        Whether to run recursively gather object in a nested list/tuple/dictionary of objects from all devices. This should only be enabled if users are not just returning tensors, and this is actively discouraged by PyTorch.\n",
       "\n",
       "    use_liger_kernel (`bool`, *optional*, defaults to `False`):\n",
       "        Whether enable [Liger](https://github.com/linkedin/Liger-Kernel) Kernel for LLM model training.\n",
       "        It can effectively increase multi-GPU training throughput by ~20% and reduces memory usage by ~60%, works out of the box with\n",
       "        flash attention, PyTorch FSDP, and Microsoft DeepSpeed. Currently, it supports llama, mistral, mixtral and gemma models.\n",
       "\n",
       "    liger_kernel_config (`Optional[dict]`, *optional*):\n",
       "        Configuration to be used for Liger Kernel. When use_liger_kernel=True, this dict is passed as keyword arguments to the\n",
       "        `_apply_liger_kernel_to_instance` function, which specifies which kernels to apply. Available options vary by model but typically\n",
       "        include: 'rope', 'swiglu', 'cross_entropy', 'fused_linear_cross_entropy', 'rms_norm', etc. If `None`, use the default kernel configurations.\n",
       "\n",
       "    average_tokens_across_devices (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to average tokens across devices. If enabled, will use all_reduce to synchronize\n",
       "        num_tokens_in_batch for precise loss calculation. Reference:\n",
       "        https://github.com/huggingface/transformers/issues/34242\n",
       "\u001b[31mFile:\u001b[39m           c:\\users\\thinkpad\\anaconda3\\envs\\peft\\lib\\site-packages\\transformers\\training_args.py\n",
       "\u001b[31mType:\u001b[39m           type\n",
       "\u001b[31mSubclasses:\u001b[39m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TrainingArguments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "490b0e84-93fc-4a22-b673-e6e933020abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=0,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.NO,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_revision=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "liger_kernel_config=None,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/data/models/bert-base-cased-finetune-yelp-300\\runs\\Jul24_17-13-09_ThinkPad,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/data/models/bert-base-cased-finetune-yelp-300,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/data/models/bert-base-cased-finetune-yelp-300,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 完整的超参数配置\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a59eed7-0d21-4ef3-afd2-1749fe9432ea",
   "metadata": {},
   "source": [
    "### 训练过程中的指标评估（Evaluate)\n",
    "\n",
    "**[Hugging Face Evaluate 库](https://huggingface.co/docs/evaluate/index)** 支持使用一行代码，获得数十种不同领域（自然语言处理、计算机视觉、强化学习等）的评估方法。 当前支持 **完整评估指标：https://huggingface.co/evaluate-metric**\n",
    "\n",
    "训练器（Trainer）在训练过程中不会自动评估模型性能。因此，我们需要向训练器传递一个函数来计算和报告指标。 \n",
    "\n",
    "Evaluate库提供了一个简单的准确率函数，您可以使用`evaluate.load`函数加载\n",
    "\n",
    "不能科学上网，可从 https://github.com/huggingface/evaluate 下载源码，然后在evaluate.load的时候，指定使用的metrics。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2c90440-af98-43c5-ad2e-76a108890bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2970afa-fbbe-4bf5-87be-95e3eba9e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"/data/models/evaluate-0.4.5/metrics/accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4010b04c-d19c-4cab-a5a0-09d63624eb1b",
   "metadata": {},
   "source": [
    "接着，调用 `compute` 函数来计算预测的准确率。\n",
    "\n",
    "在将预测传递给 compute 函数之前，我们需要将 logits 转换为预测值（**所有Transformers 模型都返回 logits**）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "781517ba-1dee-4d18-b435-ff29b4407e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc66c519-33b5-4a01-ae10-6797c38a5ab2",
   "metadata": {},
   "source": [
    "#### 训练过程指标监控\n",
    "\n",
    "通常，为了监控训练过程中的评估指标变化，我们可以在`TrainingArguments`指定`evaluation_strategy`参数，以便在 epoch 结束时报告评估指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "908cc6b6-41c8-41d9-b455-ee84b453903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=model_dir,\n",
    "                                  #evaluation_strategy=\"epoch\", \n",
    "                                  eval_strategy=\"epoch\",\n",
    "                                  per_device_train_batch_size=16,\n",
    "                                  num_train_epochs=3,\n",
    "                                  logging_steps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd77f2bc-c887-4150-acba-f13efc8b961c",
   "metadata": {},
   "source": [
    "## 开始训练\n",
    "\n",
    "### 实例化训练器（Trainer）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0ce566-cb85-4f54-bb8e-bd922e021c25",
   "metadata": {},
   "source": [
    "需要升级accelerate\n",
    "```bash\n",
    "pip uninstall accelerate -y\n",
    "pip cache purge\n",
    "pip install accelerate --upgrade\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc1c77ab-31dc-470c-926c-cf3ccb3cd4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77580462-91bc-424c-9620-538f8a3dc837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 37:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.647452</td>\n",
       "      <td>0.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.660595</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.653814</td>\n",
       "      <td>0.210000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=21, training_loss=1.5462110610235305, metrics={'train_runtime': 2358.2756, 'train_samples_per_second': 0.127, 'train_steps_per_second': 0.009, 'total_flos': 78935442739200.0, 'train_loss': 1.5462110610235305, 'epoch': 3.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b1008a-6e85-47b2-b482-c6e8bce02329",
   "metadata": {},
   "source": [
    "这是老师给的1000个样本的训练结果，可见样本增加，精度会提高。\n",
    "\n",
    "TrainOutput(global_step=189, training_loss=0.9693943861300353, metrics={'train_runtime': 341.7098, 'train_samples_per_second': 8.779, 'train_steps_per_second': 0.553, 'total_flos': 789354427392000.0, 'train_loss': 0.9693943861300353, 'epoch': 3.0})\n",
    "\n",
    "|Epoch|Training Loss|Validation Loss|Accuracy|\n",
    "|-|-|-|-|\n",
    "|1|1.242100|1.090886|0.526000|\n",
    "|2|0.901400|0.960115|0.591000|\n",
    "|3|0.638200|0.978361|0.592000|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "984d9af7-15dd-4d76-be73-8bde9a7ab74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=64).select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6a30c90-bd7b-4c91-9ed1-7f3a3fed8ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.5807993412017822,\n",
       " 'eval_accuracy': 0.3,\n",
       " 'eval_runtime': 23.2638,\n",
       " 'eval_samples_per_second': 0.43,\n",
       " 'eval_steps_per_second': 0.086,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(small_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe932427-c46b-4ae2-af80-20b82a8cfbab",
   "metadata": {},
   "source": [
    "老师给的使用1000个样本训练，使用100个随机样本测试，结果如下\n",
    "```\n",
    "{'eval_loss': 1.0753791332244873,\n",
    " 'eval_accuracy': 0.52,\n",
    " 'eval_runtime': 2.9889,\n",
    " 'eval_samples_per_second': 33.457,\n",
    " 'eval_steps_per_second': 4.349,\n",
    " 'epoch': 3.0}\n",
    "```\n",
    " **说明样本量越大，训练的精度会同步提升。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb69f6fa-ef93-46cb-a546-36be2ef2fe81",
   "metadata": {},
   "source": [
    "### 保存模型和训练状态\n",
    "\n",
    "- 使用 `trainer.save_model` 方法保存模型，后续可以通过 from_pretrained() 方法重新加载\n",
    "- 使用 `trainer.save_state` 方法保存训练状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc0cce03-9177-4a55-8673-5ba84ce73fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fecd1cb9-5d75-4517-b0a4-9e548a3e5a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ef5f0-4348-4fa2-9688-f31a670b2294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbc75642-03ed-4b86-acfe-493efa9e969f",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers 微调语言模型-问答任务\n",
    "\n",
    "我们已经学会使用 Pipeline 加载支持问答任务的预训练模型，本教程代码将展示如何微调训练一个支持问答任务的模型。\n",
    "\n",
    "**注意：微调后的模型仍然是通过提取上下文的子串来回答问题的，而不是生成新的文本。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80633cd0-b73a-4a7d-acd5-d1759e1fffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据你使用的模型和GPU资源情况，调整以下关键参数\n",
    "squad_v2 = False\n",
    "model_checkpoint = \"/data/models/distilbert--distilbert-base-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceea613d-6271-49de-adc6-0de961cd6be6",
   "metadata": {},
   "source": [
    "## 下载数据集\n",
    "\n",
    "在本教程中，我们将使用[斯坦福问答数据集(SQuAD）](https://rajpurkar.github.io/SQuAD-explorer/)。\n",
    "\n",
    "https://hf-mirror.com/datasets/rajpurkar/squad_v2\n",
    "\n",
    "### SQuAD 数据集\n",
    "\n",
    "**斯坦福问答数据集(SQuAD)** 是一个阅读理解数据集，由众包工作者在一系列维基百科文章上提出问题组成。每个问题的答案都是相应阅读段落中的文本片段或范围，或者该问题可能无法回答。\n",
    "\n",
    "SQuAD2.0将SQuAD1.1中的10万个问题与由众包工作者对抗性地撰写的5万多个无法回答的问题相结合，使其看起来与可回答的问题类似。要在SQuAD2.0上表现良好，系统不仅必须在可能时回答问题，还必须确定段落中没有支持任何答案，并放弃回答。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5253b324-b5d9-4359-b856-485a9010a762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f214adb6-3234-4a4b-a5e9-909bad5f61a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad13a5becb14ced972147667b757fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d1570674fd4f7cb926cbdf70c7adc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "squad_v2_train_full_ds = Dataset.from_parquet(\"r_data/squad_v2/train-00000-of-00001.parquet\")\n",
    "squad_v2_valid_full_ds = Dataset.from_parquet(\"r_data/squad_v2/validation-00000-of-00001.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f79bd353-fab0-46d8-819a-a4f979bfc4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = DatasetDict({\n",
    "    'train': squad_v2_train_full_ds,\n",
    "    'validation': squad_v2_valid_full_ds\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f00240a-857a-4b83-a256-1cf491d2cb19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 130319\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 11873\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b352e78-f20b-43e3-8920-55c4fe9c35a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56be85543aeaaa14008c9063',\n",
       " 'title': 'Beyoncé',\n",
       " 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
       " 'question': 'When did Beyonce start becoming popular?',\n",
       " 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d1d6f93-eb9c-4573-9bee-06ed596e2108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64583388-b709-45fb-a494-7828ed5712c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5a84d5727cf838001a46aaec</td>\n",
       "      <td>Party_leaders_of_the_United_States_House_of_Representatives</td>\n",
       "      <td>When the White House is controlled by the House majority party, then the House minority leader assumes a larger role in formulating alternatives to executive branch initiatives and in acting as a national spokesperson for his or her party. \"As Minority Leader during [President Lyndon Johnson's] Democratic administration, my responsibility has been to propose Republican alternatives,\" said Minority Leader Gerald Ford, R-MI. Greatly outnumbered in the House, Minority Leader Ford devised a political strategy that allowed Republicans to offer their alternatives in a manner that provided them political protection. As Ford explained:</td>\n",
       "      <td>What role does the executive branch assume when the house is controlled by the Republicans?</td>\n",
       "      <td>{'text': [], 'answer_start': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5726b375708984140094ce35</td>\n",
       "      <td>Pope_Paul_VI</td>\n",
       "      <td>As a Cardinal, Montini journeyed to Africa (1962), where he visited Ghana, Sudan, Kenya, Congo, Rhodesia, South Africa, and Nigeria. After his journey, John XXIII gave him a private audience on his trip which lasted for several hours. In fifteen other trips he visited Brazil (1960) and the USA (1960), including New York City, Washington, DC, Chicago, the University of Notre Dame in Indiana, Boston, Philadelphia, and Baltimore. While a cardinal, he usually vacationed in Engelberg Abbey, a secluded Benedictine monastery in Switzerland.</td>\n",
       "      <td>Where did Montini like to use as a retreat as a cardinal?</td>\n",
       "      <td>{'text': ['Engelberg Abbey'], 'answer_start': [474]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>572719a45951b619008f85f7</td>\n",
       "      <td>Capacitor</td>\n",
       "      <td>Capacitors may have their connecting leads arranged in many configurations, for example axially or radially. \"Axial\" means that the leads are on a common axis, typically the axis of the capacitor's cylindrical body – the leads extend from opposite ends. Radial leads might more accurately be referred to as tandem; they are rarely actually aligned along radii of the body's circle, so the term is inexact, although universal. The leads (until bent) are usually in planes parallel to that of the flat body of the capacitor, and extend in the same direction; they are often parallel as manufactured.</td>\n",
       "      <td>What is one type of configuration in which a capacitor may have its connecting leads organized?</td>\n",
       "      <td>{'text': ['axially'], 'answer_start': [88]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5ad370ad604f3c001a3fe25e</td>\n",
       "      <td>Separation_of_powers_under_the_United_States_Constitution</td>\n",
       "      <td>Separation of powers is a political doctrine originating in the writings of Montesquieu in The Spirit of the Laws where he urged for a constitutional government with three separate branches of government. Each of the three branches would have defined abilities to check the powers of the other branches. This idea was called separation of powers. This philosophy heavily influenced the writing of the United States Constitution, according to which the Legislative, Executive, and Judicial branches of the United States government are kept distinct in order to prevent abuse of power. This United States form of separation of powers is associated with a system of checks and balances.</td>\n",
       "      <td>Who urged for a government with four individual branches?</td>\n",
       "      <td>{'text': [], 'answer_start': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5731dbdf0fdd8d15006c65a6</td>\n",
       "      <td>Bras%C3%ADlia</td>\n",
       "      <td>Brasília has a tropical savanna climate (Aw) according to the Köppen system, with two distinct seasons: the rainy season, from October to April, and a dry season, from May to September. The average temperature is 20.6 °C (69.1 °F). September, at the end of the dry season, has the highest average maximum temperature, 28.3 °C (82.9 °F), has major and minor lower maximum average temperature, of 25.1 °C (77.2 °F) and 12.9 °C (55.2 °F), respectively. Average temperatures from September through March are a consistent 22 °C (72 °F). With 247.4 mm (9.7 in), January is the month with the highest rainfall of the year, while June is the lowest, with only 8.7 mm (0.3 in).</td>\n",
       "      <td>When is Brasilia's rainy season?</td>\n",
       "      <td>{'text': ['October to April'], 'answer_start': [127]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>572fe9d104bcaa1900d76ea0</td>\n",
       "      <td>Premier_League</td>\n",
       "      <td>There has been an increasing gulf between the Premier League and the Football League. Since its split with the Football League, many established clubs in the Premier League have managed to distance themselves from their counterparts in lower leagues. Owing in large part to the disparity in revenue from television rights between the leagues, many newly promoted teams have found it difficult to avoid relegation in their first season in the Premier League. In every season except 2001–02 and 2011–12, at least one Premier League newcomer has been relegated back to the Football League. In 1997–98 all three promoted clubs were relegated at the end of the season.</td>\n",
       "      <td>Did many new teams in the Premier League have any difficulties in their initial season?</td>\n",
       "      <td>{'text': ['many newly promoted teams have found it difficult to avoid relegation in their first season in the Premier League.'], 'answer_start': [343]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5a32e7c0bd0d7f001aef94ef</td>\n",
       "      <td>Central_African_Republic</td>\n",
       "      <td>In 2004 the Central African Republic Bush War began as forces opposed to Bozizé took up arms against his government. In May 2005 Bozizé won a presidential election that excluded Patassé and in 2006 fighting continued between the government and the rebels. In November 2006, Bozizé's government requested French military support to help them repel rebels who had taken control of towns in the country's northern regions. Though the initially public details of the agreement pertained to logistics and intelligence, the French assistance eventually included strikes by Mirage jets against rebel positions.</td>\n",
       "      <td>How did Patasse help Bozize against the rebels?</td>\n",
       "      <td>{'text': [], 'answer_start': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5706872b52bb891400689a44</td>\n",
       "      <td>Bird_migration</td>\n",
       "      <td>Some large broad-winged birds rely on thermal columns of rising hot air to enable them to soar. These include many birds of prey such as vultures, eagles, and buzzards, but also storks. These birds migrate in the daytime. Migratory species in these groups have great difficulty crossing large bodies of water, since thermals only form over land, and these birds cannot maintain active flight for long distances. Mediterranean and other seas present a major obstacle to soaring birds, which must cross at the narrowest points. Massive numbers of large raptors and storks pass through areas such as the Strait of Messina, Gibraltar, Falsterbo, and the Bosphorus at migration times. More common species, such as the European honey buzzard Pernis apivorus, can be counted in hundreds of thousands in autumn. Other barriers, such as mountain ranges, can also cause funnelling, particularly of large diurnal migrants. This is a notable factor in the Central American migratory bottleneck. Batumi bottleneck in the Caucasus is one of the heaviest migratory funnels on earth. Avoiding flying over the Black Sea surface and across high mountains, hundreds of thousands of soaring birds funnel through an area around the city of Batumi, Georgia. Birds of prey such as honey buzzards which migrate using thermals lose only 10 to 20% of their weight during migration, which may explain why they forage less during migration than do smaller birds of prey with more active flight such as falcons, hawks and harriers.</td>\n",
       "      <td>What do some large broad-winged birds rely on to help them soar?</td>\n",
       "      <td>{'text': ['thermal columns of rising hot air'], 'answer_start': [38]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5725d5a3271a42140099d27d</td>\n",
       "      <td>Montevideo</td>\n",
       "      <td>The other large cemeteries are the Cementerio del Buceo, Cementerio del Cerro, and Cementerio Paso Molino. The British Cemetery Montevideo (Cementerio Británico) is another of the oldest cemeteries in Uruguay, located in the Buceo neighborhood. Many noblemen and eminent persons are buried there. The cemetery originated when the Englishman Mr. Thomas Samuel Hood purchased a plot of land in the name of the English residents in 1828. However, in 1884 the government compensated the British by moving the cemetery to Buceo to accommodate city growth. A section of the cemetery, known as British Cemetery Montevideo Soldiers and Sailors, contains the graves of quite a number of sailors of different nationalities, although the majority are of British descent. One United States Marine, Henry de Costa, is buried here.</td>\n",
       "      <td>Where is the British Cemetery Montevideo located?</td>\n",
       "      <td>{'text': ['Buceo neighborhood'], 'answer_start': [225]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>56ddc47666d3e219004dacdd</td>\n",
       "      <td>Wayback_Machine</td>\n",
       "      <td>In March 2011, it was said on the Wayback Machine forum that \"The Beta of the new Wayback Machine has a more complete and up-to-date index of all crawled materials into 2010, and will continue to be updated regularly. The index driving the classic Wayback Machine only has a little bit of material past 2008, and no further index updates are planned, as it will be phased out this year\".</td>\n",
       "      <td>When were details of the test version of the updated Wayback Machine released?</td>\n",
       "      <td>{'text': ['March 2011'], 'answer_start': [3]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c127cdc6-1ba6-4525-968b-e369443da793",
   "metadata": {},
   "source": [
    "## 预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e611e1d5-d0bd-4efc-adb1-d38c6bc7bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bd4986-714b-480b-8597-fac4def46833",
   "metadata": {},
   "source": [
    "以下断言确保我们的 Tokenizers 使用的是 FastTokenizer（Rust 实现，速度和功能性上有一定优势）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60401591-7667-4504-81fb-b58cfb4e03c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9384f32-3f12-40a8-8813-c2e0a011f730",
   "metadata": {},
   "source": [
    "您可以在大模型表上查看哪种类型的模型具有可用的快速标记器，哪种类型没有。\n",
    "\n",
    "您可以直接在两个句子上调用此标记器（一个用于答案，一个用于上下文）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "032e78c5-55d8-4041-9814-b8af11ec1116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2054, 2003, 2115, 2171, 1029, 102, 2026, 2171, 2003, 25353, 22144, 2378, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"What is your name?\", \"My name is Sylvain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c030fa2f-33ab-4565-a8cf-2e7d88029b3d",
   "metadata": {},
   "source": [
    "### Tokenizer 进阶操作\n",
    "\n",
    "在问答预处理中的一个特定问题是如何处理非常长的文档。\n",
    "\n",
    "在其他任务中，当文档的长度超过模型最大句子长度时，我们通常会截断它们，但在这里，删除上下文的一部分可能会导致我们丢失正在寻找的答案。\n",
    "\n",
    "为了解决这个问题，我们允许数据集中的一个（长）示例生成多个输入特征，每个特征的长度都小于模型的最大长度（或我们设置的超参数）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4ccbc45-4da7-49d3-b4a6-b08866eb690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length of a feature (question and context)\n",
    "max_length = 384 \n",
    "# The authorized overlap between two part of the context when splitting it is needed.\n",
    "doc_stride = 128 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22275fb-b7a8-49f6-9416-6b9cb4f3a102",
   "metadata": {},
   "source": [
    "#### 超出最大长度的文本数据处理\n",
    "\n",
    "下面，我们从训练集中找出一个超过最大长度（384）的文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bfdeba5-8bb5-4953-b8d7-b78311d12c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example in enumerate(datasets[\"train\"]):\n",
    "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n",
    "        break\n",
    "# 挑选出来超过384（最大长度）的数据样例\n",
    "example = datasets[\"train\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9822af51-3fa3-48e2-bf02-adc5029437fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "437"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e1b202-e61b-4abc-979e-16d0408174be",
   "metadata": {},
   "source": [
    "#### 截断上下文不保留超出部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00fcd67f-42a7-4038-bf24-1427e7d2455f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"],\n",
    "              example[\"context\"],\n",
    "              max_length=max_length,\n",
    "              truncation=\"only_second\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26766b7-8a54-485b-9ac6-392ac1898371",
   "metadata": {},
   "source": [
    "#### 关于截断的策略\n",
    "\n",
    "- 直接截断超出部分: truncation=`only_second`\n",
    "- 仅截断上下文（context），保留问题（question）：`return_overflowing_tokens=True` & 设置`stride`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd63bef2-4fd5-42f0-b546-46639b90a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=doc_stride\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6396421-47c0-4502-886b-85e9366ddf6b",
   "metadata": {},
   "source": [
    "使用此策略截断后，Tokenizer 将返回多个 `input_ids` 列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e25b4c2-2b1e-468c-92fc-14612e9e85b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[384, 192]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in tokenized_example[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b4985-b9c6-4b69-8dd0-685bc33b39e9",
   "metadata": {},
   "source": [
    "解码两个输入特征，可以看到重叠的部分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42977aac-914d-47b1-b5bc-4287fa20fdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] beyonce got married in 2008 to whom? [SEP] on april 4, 2008, beyonce married jay z. she publicly revealed their marriage in a video montage at the listening party for her third studio album, i am... sasha fierce, in manhattan ' s sony club on october 22, 2008. i am... sasha fierce was released on november 18, 2008 in the united states. the album formally introduces beyonce ' s alter ego sasha fierce, conceived during the making of her 2003 single \" crazy in love \", selling 482, 000 copies in its first week, debuting atop the billboard 200, and giving beyonce her third consecutive number - one album in the us. the album featured the number - one song \" single ladies ( put a ring on it ) \" and the top - five songs \" if i were a boy \" and \" halo \". achieving the accomplishment of becoming her longest - running hot 100 single in her career, \" halo \" ' s success in the us helped beyonce attain more top - ten singles on the list than any other woman during the 2000s. it also included the successful \" sweet dreams \", and singles \" diva \", \" ego \", \" broken - hearted girl \" and \" video phone \". the music video for \" single ladies \" has been parodied and imitated around the world, spawning the \" first major dance craze \" of the internet age according to the toronto star. the video has won several awards, including best video at the 2009 mtv europe music awards, the 2009 scottish mobo awards, and the 2009 bet awards. at the 2009 mtv video music awards, the video was nominated for nine awards, ultimately winning three including video of the year. its failure to win the best female video category, which went to american country pop singer taylor swift ' s \" you belong with me \", led to kanye west interrupting the ceremony and beyonce [SEP]\n",
      "[CLS] beyonce got married in 2008 to whom? [SEP] single ladies \" has been parodied and imitated around the world, spawning the \" first major dance craze \" of the internet age according to the toronto star. the video has won several awards, including best video at the 2009 mtv europe music awards, the 2009 scottish mobo awards, and the 2009 bet awards. at the 2009 mtv video music awards, the video was nominated for nine awards, ultimately winning three including video of the year. its failure to win the best female video category, which went to american country pop singer taylor swift ' s \" you belong with me \", led to kanye west interrupting the ceremony and beyonce improvising a re - presentation of swift ' s award during her own acceptance speech. in march 2009, beyonce embarked on the i am... world tour, her second headlining worldwide concert tour, consisting of 108 shows, grossing $ 119. 5 million. [SEP]\n"
     ]
    }
   ],
   "source": [
    "for x in tokenized_example[\"input_ids\"][:2]:\n",
    "    print(tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b9b726-c234-40f9-a652-e822f9e87155",
   "metadata": {},
   "source": [
    "#### 使用 offsets_mapping 获取原始的 input_ids\n",
    "\n",
    "设置 `return_offsets_mapping=True`，将使得截断分割生成的多个 input_ids 列表中的 token，通过映射保留原始文本的 input_ids。\n",
    "\n",
    "如下所示：第一个标记（[CLS]）的起始和结束字符都是（0, 0），因为它不对应问题/答案的任何部分，然后第二个标记与问题(question)的字符0到3相同."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25da32bd-fa82-440a-8b07-d657c5f5de4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 7), (8, 11), (12, 19), (20, 22), (23, 27), (28, 30), (31, 35), (35, 36), (0, 0), (0, 2), (3, 8), (9, 10), (10, 11), (12, 16), (16, 17), (18, 25), (26, 33), (34, 37), (38, 39), (39, 40), (41, 44), (45, 53), (54, 62), (63, 68), (69, 77), (78, 80), (81, 82), (83, 88), (89, 93), (93, 96), (97, 99), (100, 103), (104, 113), (114, 119), (120, 123), (124, 127), (128, 133), (134, 140), (141, 146), (146, 147), (148, 149), (150, 152), (152, 153), (153, 154), (154, 155), (156, 161), (162, 168), (168, 169), (170, 172), (173, 182), (182, 183), (183, 184), (185, 189), (190, 194), (195, 197), (198, 205), (206, 208), (208, 209), (210, 214), (214, 215), (216, 217), (218, 220), (220, 221), (221, 222), (222, 223), (224, 229), (230, 236), (237, 240), (241, 249), (250, 252), (253, 261), (262, 264), (264, 265), (266, 270), (271, 273), (274, 277), (278, 284), (285, 291), (291, 292), (293, 296), (297, 302), (303, 311), (312, 322), (323, 330), (330, 331), (331, 332), (333, 338), (339, 342), (343, 348), (349, 355), (355, 356), (357, 366), (367, 373), (374, 377), (378, 384), (385, 387), (388, 391), (392, 396), (397, 403)]\n"
     ]
    }
   ],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    stride=doc_stride\n",
    ")\n",
    "print(tokenized_example[\"offset_mapping\"][0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6335297-4f25-47a5-9e15-c0027ddc1237",
   "metadata": {},
   "source": [
    "因此，我们可以使用这个映射来找到答案在给定特征中的起始和结束标记的位置。\n",
    "\n",
    "我们只需区分偏移的哪些部分对应于问题，哪些部分对应于上下文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bc03239-fd58-4634-971a-403868dcca24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beyonce Beyonce\n"
     ]
    }
   ],
   "source": [
    "first_token_id = tokenized_example[\"input_ids\"][0][1]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
    "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd9076d3-46d6-405b-9a08-0aaef7306742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got got\n"
     ]
    }
   ],
   "source": [
    "second_token_id = tokenized_example[\"input_ids\"][0][2]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][2]\n",
    "print(tokenizer.convert_ids_to_tokens([second_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7e3246a-d156-4306-b69a-498fcf03dc37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beyonce got married in 2008 to whom?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"question\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ecd00f-aadc-4613-b15e-6fe7777cded1",
   "metadata": {},
   "source": [
    "借助`tokenized_example`的`sequence_ids`方法，我们可以方便的区分token的来源编号：\n",
    "\n",
    "- 对于特殊标记：返回None，\n",
    "- 对于正文Token：返回句子编号（从0开始编号）。\n",
    "\n",
    "综上，现在我们可以很方便的在一个输入特征中找到答案的起始和结束 Token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba0c45b8-ac78-4607-b10d-88a827274843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "sequence_ids = tokenized_example.sequence_ids()\n",
    "print(sequence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "914117b5-aaf8-4c6f-913a-278d6a60d4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 19\n"
     ]
    }
   ],
   "source": [
    "answers = example[\"answers\"]\n",
    "start_char = answers[\"answer_start\"][0]\n",
    "end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "# 当前span在文本中的起始标记索引。\n",
    "token_start_index = 0\n",
    "while sequence_ids[token_start_index] != 1:\n",
    "    token_start_index += 1\n",
    "\n",
    "# 当前span在文本中的结束标记索引。\n",
    "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n",
    "while sequence_ids[token_end_index] != 1:\n",
    "    token_end_index -= 1\n",
    "\n",
    "# 检测答案是否超出span范围（如果超出范围，该特征将以CLS标记索引标记）。\n",
    "offsets = tokenized_example[\"offset_mapping\"][0]\n",
    "if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "    # 将token_start_index和token_end_index移动到答案的两端。\n",
    "    # 注意：如果答案是最后一个单词，我们可以移到最后一个标记之后（边界情况）。\n",
    "    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "        token_start_index += 1\n",
    "    start_position = token_start_index - 1\n",
    "    while offsets[token_end_index][1] >= end_char:\n",
    "        token_end_index -= 1\n",
    "    end_position = token_end_index + 1\n",
    "    print(start_position, end_position)\n",
    "else:\n",
    "    print(\"答案不在此特征中。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613dbc23-d74f-4517-ab68-e71d3a907864",
   "metadata": {},
   "source": [
    "打印检查是否准确找到了起始位置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7feb58eb-99a0-4231-bd92-cfb5caa4594b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jay z\n",
      "Jay Z\n"
     ]
    }
   ],
   "source": [
    "# 通过查找 offset mapping 位置，解码 context 中的答案 \n",
    "print(tokenizer.decode(tokenized_example[\"input_ids\"][0][start_position: end_position+1]))\n",
    "# 直接打印 数据集中的标准答案（answer[\"text\"])\n",
    "print(answers[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af85e409-0113-4230-bdd7-4c4093a3e8bd",
   "metadata": {},
   "source": [
    "#### 关于填充的策略\n",
    "\n",
    "- 对于没有超过最大长度的文本，填充补齐长度。\n",
    "- 对于需要左侧填充的模型，交换 question 和 context 顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a5ebc4a-a044-4eeb-88d5-a2637ba48d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_on_right = tokenizer.padding_side == \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390eba2e-0595-4ae9-bb98-ba952d6253f8",
   "metadata": {},
   "source": [
    "### 整合以上所有预处理步骤\n",
    "\n",
    "让我们将所有内容整合到一个函数中，并将其应用到训练集。\n",
    "\n",
    "针对不可回答的情况（上下文过长，答案在另一个特征中），我们为开始和结束位置都设置了cls索引。\n",
    "\n",
    "如果allow_impossible_answers标志为False，我们还可以简单地从训练集中丢弃这些示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4a7d171-9f3b-49fc-8b42-86a5676dc4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # 一些问题的左侧可能有很多空白字符，这对我们没有用，而且会导致上下文的截断失败\n",
    "    # （标记化的问题将占用大量空间）。因此，我们删除左侧的空白字符。\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # 使用截断和填充对我们的示例进行标记化，但保留溢出部分，使用步幅（stride）。\n",
    "    # 当上下文很长时，这会导致一个示例可能提供多个特征，其中每个特征的上下文都与前一个特征的上下文有一些重叠。\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # 由于一个示例可能给我们提供多个特征（如果它具有很长的上下文），我们需要一个从特征到其对应示例的映射。这个键就提供了这个映射关系。\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # 偏移映射将为我们提供从令牌到原始上下文中的字符位置的映射。这将帮助我们计算开始位置和结束位置。\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # 让我们为这些示例进行标记！\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # 我们将使用 CLS 特殊 token 的索引来标记不可能的答案。\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # 获取与该示例对应的序列（以了解上下文和问题是什么）。\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # 一个示例可以提供多个跨度，这是包含此文本跨度的示例的索引。\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # 如果没有给出答案，则将cls_index设置为答案。\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # 答案在文本中的开始和结束字符索引。\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # 当前跨度在文本中的开始令牌索引。\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # 当前跨度在文本中的结束令牌索引。\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # 检测答案是否超出跨度（在这种情况下，该特征的标签将使用CLS索引）。\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # 否则，将token_start_index和token_end_index移到答案的两端。\n",
    "                # 注意：如果答案是最后一个单词（边缘情况），我们可以在最后一个偏移之后继续。\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0746e86-8e85-48bd-ae41-821a1c32dd62",
   "metadata": {},
   "source": [
    "#### datasets.map 的进阶使用\n",
    "\n",
    "使用 `datasets.map` 方法将 `prepare_train_features` 应用于所有训练、验证和测试数据：\n",
    "\n",
    "- batched: 批量处理数据。\n",
    "- remove_columns: 因为预处理更改了样本的数量，所以在应用它时需要删除旧列。\n",
    "- load_from_cache_file：是否使用datasets库的自动缓存\n",
    "\n",
    "datasets 库针对大规模数据，实现了高效缓存机制，能够自动检测传递给 map 的函数是否已更改（因此需要不使用缓存数据）。如果在调用 map 时设置 `load_from_cache_file=False`，可以强制重新应用预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec8b6400-f689-4fec-8ff3-fe2689fd701f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa30704d7e5548a6b455951646899e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d9c6c50049b4046a4f5c2a0a082623b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(prepare_train_features,\n",
    "                                  batched=True,\n",
    "                                  remove_columns=datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916b2f39-7ea2-4de7-b0c6-48956149c610",
   "metadata": {},
   "source": [
    "## 微调模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135eaa34-e1da-4ab7-b090-dbecb4da2d4f",
   "metadata": {},
   "source": [
    "现在我们的数据已经准备好用于训练，我们可以下载预训练模型并进行微调。\n",
    "\n",
    "由于我们的任务是问答，我们使用 `AutoModelForQuestionAnswering` 类。(对比 Yelp 评论打分使用的是 `AutoModelForSequenceClassification` 类）\n",
    "\n",
    "警告通知我们正在丢弃一些权重（`vocab_transform` 和 `vocab_layer_norm` 层），并随机初始化其他一些权重（`pre_classifier` 和 `classifier` 层）。在微调模型情况下是绝对正常的，因为我们正在删除用于预训练模型的掩码语言建模任务的头部，并用一个新的头部替换它，对于这个新头部，我们没有预训练的权重，所以库会警告我们在用它进行推理之前应该对这个模型进行微调，而这正是我们要做的事情。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54ddb235-dcbd-46b6-accd-84454bd524ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at /data/models/distilbert--distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4743f0-a2fa-4f3f-bfc5-36ddb121f254",
   "metadata": {},
   "source": [
    "#### 训练超参数（TrainingArguments）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b88c72ac-1571-40d1-a812-119fdca82a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "model_dir = f\"/data/models/distilbert--distilbert-base-uncased-finetuned-squad\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    eval_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d581ea7f-fa76-408c-80bd-0e9f4c6f09ad",
   "metadata": {},
   "source": [
    "#### Data Collator（数据整理器）\n",
    "\n",
    "数据整理器将训练数据整理为批次数据，用于模型训练时的批次处理。本教程使用默认的 `default_data_collator`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65251c19-bf72-496c-ad09-823260e15145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07a1889-6c34-4242-a4d6-2b123e08142e",
   "metadata": {},
   "source": [
    "### 实例化训练器（Trainer）\n",
    "\n",
    "为了减少训练时间（需要大量算力支持），我们不在本教程的训练模型过程中计算模型评估指标。\n",
    "\n",
    "而是训练完成后，再独立进行模型评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "935ae7b4-30c5-43cd-881a-546342f5044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当前没有算力，仅笔记本CPU来运行，先缩小训练集合大小，了解整个微调过程。\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=66).select(range(100))\n",
    "small_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=66).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be1ac235-65c1-4aba-a43b-40e477179e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ThinkPad\\AppData\\Local\\Temp\\ipykernel_19252\\4021240430.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83be648d-db90-4904-b6aa-49d2f9b435cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 10:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.990912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.942616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.922647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=5.942782084147136, metrics={'train_runtime': 763.8786, 'train_samples_per_second': 0.393, 'train_steps_per_second': 0.008, 'total_flos': 29396947507200.0, 'train_loss': 5.942782084147136, 'epoch': 3.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cfc950d8-ed19-4793-9d49-876a2459aaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40600348-b81c-4b8a-8c91-4a1ba07d36f9",
   "metadata": {},
   "source": [
    "## 模型评估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7917c72-2f5c-42b6-8020-e9c46a3390e0",
   "metadata": {},
   "source": [
    "**评估模型输出需要一些额外的处理：将模型的预测映射回上下文的部分。**\n",
    "\n",
    "模型直接输出的是预测答案的`起始位置`和`结束位置`的**logits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db5fa4c2-e4b1-43ad-88c1-04bc810aaca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'start_logits', 'end_logits'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "for batch in trainer.get_eval_dataloader():\n",
    "    break\n",
    "batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n",
    "with torch.no_grad():\n",
    "    output = trainer.model(**batch)\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8354a12e-c26d-4158-8181-22d5da9f3915",
   "metadata": {},
   "source": [
    "模型的输出是一个类似字典的对象，其中包含损失（因为我们提供了标签），以及起始和结束logits。我们不需要损失来进行预测，让我们看一下logits："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a80c4751-65dd-4288-a85f-52670ed9b096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 384]), torch.Size([64, 384]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.shape, output.end_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4033bec-4045-40d2-b1e6-e0c3f2b3d7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([121,  64,  47, 142,  63,  72,  94, 116,  24,  38, 134, 125,  99,  14,\n",
       "         156,  27, 120,  45,  38,  72, 129,  27,  43,  22,  10,  68, 126, 107,\n",
       "         121,  98,  29, 160, 210, 138,  22,  69, 331,  19,  16,  39, 101, 233,\n",
       "          75, 101,  60,  50,  46,  68,  28,  83,  71, 106, 155,  64, 150,   4,\n",
       "          87, 105,  37, 163,  18,  84, 144,   3]),\n",
       " tensor([  0, 177, 105, 120,  77,  24, 266, 120, 125, 162, 106, 113,  35, 133,\n",
       "         132,  51,  17, 125, 136, 175,   0, 262, 110, 115,  72, 153, 127, 134,\n",
       "         112,   0,  78, 110, 126,  36,  45, 290, 127, 159, 130, 109, 326,  31,\n",
       "          12,  22,   4, 109, 138, 130, 161, 126, 269,  37,  97, 132, 128, 140,\n",
       "          96, 152, 167,  27, 132, 101, 116, 117]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1895ebb-eefe-4b6b-bf24-de2d4cce9d53",
   "metadata": {},
   "source": [
    "#### 如何从模型输出的位置 logit 组合成答案\n",
    "\n",
    "我们有每个特征和每个标记的logit。在每个特征中为每个标记预测答案最明显的方法是，将起始logits的最大索引作为起始位置，将结束logits的最大索引作为结束位置。\n",
    "\n",
    "在许多情况下这种方式效果很好，但是如果此预测给出了不可能的结果该怎么办？比如：起始位置可能大于结束位置，或者指向问题中的文本片段而不是答案。在这种情况下，我们可能希望查看第二好的预测，看它是否给出了一个可能的答案，并选择它。\n",
    "\n",
    "选择第二好的答案并不像选择最佳答案那么容易：\n",
    "- 它是起始logits中第二佳索引与结束logits中最佳索引吗？\n",
    "- 还是起始logits中最佳索引与结束logits中第二佳索引？\n",
    "- 如果第二好的答案也不可能，那么对于第三好的答案，情况会更加棘手。\n",
    "\n",
    "为了对答案进行分类，\n",
    "1. 将使用通过添加起始和结束logits获得的分数\n",
    "1. 设计一个名为`n_best_size`的超参数，限制不对所有可能的答案进行排序。\n",
    "1. 我们将选择起始和结束logits中的最佳索引，并收集这些预测的所有答案。\n",
    "1. 在检查每一个是否有效后，我们将按照其分数对它们进行排序，并保留最佳的答案。\n",
    "\n",
    "以下是我们如何在批次中的第一个特征上执行此操作的示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea3e93b7-1eff-40df-bdb9-8b599e2b9aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_best_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b247af4d-4f12-49c9-a35f-4d372e2fbb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "\n",
    "# 获取最佳的起始和结束位置的索引：\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "\n",
    "valid_answers = []\n",
    "\n",
    "# 遍历起始位置和结束位置的索引组合\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        if start_index <= end_index:  # 需要进一步测试以检查答案是否在上下文中\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": \"\"  # 我们需要找到一种方法来获取与上下文中答案对应的原始子字符串\n",
    "                }\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454cfe70-3936-4720-b64a-387d13de0054",
   "metadata": {},
   "source": [
    "然后，我们可以根据它们的得分对`valid_answers`进行排序，并仅保留最佳答案。唯一剩下的问题是如何检查给定的跨度是否在上下文中（而不是问题中），以及如何获取其中的文本。为此，我们需要向我们的验证特征添加两个内容：\n",
    "\n",
    "- 生成该特征的示例的ID（因为每个示例可以生成多个特征，如前所示）；\n",
    "- 偏移映射，它将为我们提供从标记索引到上下文中字符位置的映射。\n",
    "\n",
    "这就是为什么我们将使用以下函数稍微不同于`prepare_train_features`来重新处理验证集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "888fd6e5-3f50-44c0-8651-eadc3a311931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # 一些问题的左侧有很多空白，这些空白并不有用且会导致上下文截断失败（分词后的问题会占用很多空间）。\n",
    "    # 因此我们移除这些左侧空白\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # 使用截断和可能的填充对我们的示例进行分词，但使用步长保留溢出的令牌。这导致一个长上下文的示例可能产生\n",
    "    # 几个特征，每个特征的上下文都会稍微与前一个特征的上下文重叠。\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # 由于一个示例在上下文很长时可能会产生几个特征，我们需要一个从特征映射到其对应示例的映射。这个键就是为了这个目的。\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # 我们保留产生这个特征的示例ID，并且会存储偏移映射。\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # 获取与该示例对应的序列（以了解哪些是上下文，哪些是问题）。\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # 一个示例可以产生几个文本段，这里是包含该文本段的示例的索引。\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # 将不属于上下文的偏移映射设置为None，以便容易确定一个令牌位置是否属于上下文。\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d9e335-516d-4f10-a23c-0965eee85614",
   "metadata": {},
   "source": [
    "将`prepare_validation_features`应用到验证集： 限于算力，随机取200个样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4d0d7248-980e-4b46-9c2f-305d3c5386fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_validation_dataset_200 = datasets[\"validation\"].shuffle(seed=66).select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c275a94d-373b-49c5-bd0a-678d1b189e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9158b5cba524bec83fdb36b708bfe41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation_features = small_validation_dataset_200.map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=small_validation_dataset_200.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7749f1dd-1450-40a1-9e7c-b42232ae4065",
   "metadata": {},
   "source": [
    "Now we can grab the predictions for all features by using the `Trainer.predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cfc9a643-0a22-49e2-a0b5-5dea091e81f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictions = trainer.predict(validation_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1407a68-cc33-42d3-b1f1-323106c31107",
   "metadata": {},
   "source": [
    "`Trainer`会隐藏模型不使用的列（在这里是`example_id`和`offset_mapping`，我们需要它们进行后处理），所以我们需要将它们重新设置回来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "149968ed-7156-4de6-8c69-8dcde134f210",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6067e067-f19d-4579-a72a-214aa1c6cff1",
   "metadata": {},
   "source": [
    "现在，我们可以改进之前的测试：\n",
    "\n",
    "由于在偏移映射中，当它对应于问题的一部分时，我们将其设置为None，因此可以轻松检查答案是否完全在上下文中。我们还可以从考虑中排除非常长的答案（可以调整的超参数）。\n",
    "\n",
    "展开说下具体实现：\n",
    "- 首先从模型输出中获取起始和结束的逻辑值（logits），这些值表明答案在文本中可能开始和结束的位置。\n",
    "- 然后，它使用偏移映射（offset_mapping）来找到这些逻辑值在原始文本中的具体位置。\n",
    "- 接下来，代码遍历可能的开始和结束索引组合，排除那些不在上下文范围内或长度不合适的答案。\n",
    "- 对于有效的答案，它计算出一个分数（基于开始和结束逻辑值的和），并将答案及其分数存储起来。\n",
    "- 最后，它根据分数对答案进行排序，并返回得分最高的几个答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "26b9452c-9f87-4eb2-aa9a-e56a9dafffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_answer_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "55755023-e25f-4adf-90ca-78d91da59ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.47923446,\n",
       "  'text': ' the left banks of the Upper Rhine was sold to Burgundy by Archduke Sigismund o'},\n",
       " {'score': 0.43272084,\n",
       "  'text': 'banks of the Upper Rhine was sold to Burgundy by Archduke Sigismund o'},\n",
       " {'score': 0.4242437,\n",
       "  'text': 'of the Upper Rhine was sold to Burgundy by Archduke Sigismund o'},\n",
       " {'score': 0.4019267,\n",
       "  'text': ' the left banks of the Upper Rhine was sold to Burgundy by Archduke Sigismund of Austria in 1469 and '},\n",
       " {'score': 0.37975088,\n",
       "  'text': ' the left banks of the Upper Rhine was sold to Burgundy by Archduke'},\n",
       " {'score': 0.35541308,\n",
       "  'text': 'banks of the Upper Rhine was sold to Burgundy by Archduke Sigismund of Austria in 1469 and '},\n",
       " {'score': 0.34693593,\n",
       "  'text': 'of the Upper Rhine was sold to Burgundy by Archduke Sigismund of Austria in 1469 and '},\n",
       " {'score': 0.3404683,\n",
       "  'text': ' the left banks of the Upper Rhine was sold to Bu'},\n",
       " {'score': 0.3332373,\n",
       "  'text': 'banks of the Upper Rhine was sold to Burgundy by Archduke'},\n",
       " {'score': 0.3247601,\n",
       "  'text': 'of the Upper Rhine was sold to Burgundy by Archduke'},\n",
       " {'score': 0.29952523,\n",
       "  'text': \"banks of the Upper Rhine was sold to Burgundy by Archduke Sigismund of Austria in 1469 and eventually fell to France in the Thirty Years' War. \"},\n",
       " {'score': 0.2958842,\n",
       "  'text': 'st French Empire and its client states. The Alsace on the left banks of the Upper Rhine was sold to Burgundy by Archduke Sigismund o'},\n",
       " {'score': 0.29395467, 'text': 'banks of the Upper Rhine was sold to Bu'},\n",
       " {'score': 0.29104805,\n",
       "  'text': \"of the Upper Rhine was sold to Burgundy by Archduke Sigismund of Austria in 1469 and eventually fell to France in the Thirty Years' War. \"},\n",
       " {'score': 0.28547752, 'text': 'of the Upper Rhine was sold to Bu'},\n",
       " {'score': 0.28081644,\n",
       "  'text': 'e left banks of the Upper Rhine was sold to Burgundy by Archduke Sigismund o'},\n",
       " {'score': 0.2667638,\n",
       "  'text': 'lsace on the left banks of the Upper Rhine was sold to Burgundy by Archduke Sigismund o'},\n",
       " {'score': 0.24677177,\n",
       "  'text': 'ire and its client states. The Alsace on the left banks of the Upper Rhine was sold to Burgundy by Archduke Sigismund o'},\n",
       " {'score': 0.23801026,\n",
       "  'text': ' the Rhine, in the county of Holland, fell to the Burgundian Netherlands in the 15th century; Holland remained contentious territory t'},\n",
       " {'score': 0.20350869,\n",
       "  'text': 'e left banks of the Upper Rhine was sold to Burgundy by Archduke Sigismund of Austria in 1469 and '}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "offset_mapping = validation_features[0][\"offset_mapping\"]\n",
    "\n",
    "# 第一个特征来自第一个示例。对于更一般的情况，我们需要将example_id匹配到一个示例索引\n",
    "context = small_validation_dataset_200[4][\"context\"]\n",
    "\n",
    "# 收集最佳开始/结束逻辑的索引：\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        # 不考虑超出范围的答案，原因是索引超出范围或对应于输入ID的部分不在上下文中。\n",
    "        if (\n",
    "            start_index >= len(offset_mapping)\n",
    "            or end_index >= len(offset_mapping)\n",
    "            or offset_mapping[start_index] is None\n",
    "            or offset_mapping[end_index] is None\n",
    "        ):\n",
    "            continue\n",
    "        # 不考虑长度小于0或大于max_answer_length的答案。\n",
    "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "            continue\n",
    "        if start_index <= end_index: # 我们需要细化这个测试，以检查答案是否在上下文中\n",
    "            start_char = offset_mapping[start_index][0]\n",
    "            end_char = offset_mapping[end_index][1]\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": context[start_char: end_char]\n",
    "                }\n",
    "            )\n",
    "\n",
    "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
    "valid_answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04822d32-e7fe-4e39-9c09-666439814e9d",
   "metadata": {},
   "source": [
    "打印比较模型输出和标准答案（Ground-truth）是否一致:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "20b764ab-9458-4f35-98f7-14560de60935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Archduke Sigismund',\n",
       "  'Archduke Sigismund of Austria',\n",
       "  'Archduke Sigismund of Austria'],\n",
       " 'answer_start': [659, 659, 659]}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_validation_dataset_200[4][\"answers\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd94808d-b90b-4a79-bc37-119507045950",
   "metadata": {},
   "source": [
    "**模型最高概率的输出与标准答案一致**\n",
    "\n",
    "正如上面的代码所示，这在第一个特征上很容易，因为我们知道它来自第一个示例。\n",
    "\n",
    "对于其他特征，我们需要建立一个示例与其对应特征的映射关系。\n",
    "\n",
    "此外，由于一个示例可以生成多个特征，我们需要将由给定示例生成的所有特征中的所有答案汇集在一起，然后选择最佳答案。\n",
    "\n",
    "下面的代码构建了一个示例索引到其对应特征索引的映射关系："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "94f0f7ce-c270-429f-8db2-39036b89a975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "examples = datasets[\"validation\"]\n",
    "features = validation_features\n",
    "\n",
    "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "features_per_example = collections.defaultdict(list)\n",
    "for i, feature in enumerate(features):\n",
    "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9243ac-bc7d-4868-8081-cabff7c6a4db",
   "metadata": {},
   "source": [
    "当`squad_v2 = True`时，有一定概率出现不可能的答案（impossible answer)。\n",
    "\n",
    "上面的代码仅保留在上下文中的答案，我们还需要获取不可能答案的分数（其起始和结束索引对应于CLS标记的索引）。\n",
    "\n",
    "当一个示例生成多个特征时，我们必须在所有特征中的不可能答案都预测出现不可能答案时（因为一个特征可能之所以能够预测出不可能答案，是因为答案不在它可以访问的上下文部分），这就是为什么一个示例中不可能答案的分数是该示例生成的每个特征中的不可能答案的分数的最小值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cb941927-a352-4860-a1f1-730d6dc53d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_v2 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bd277b88-62ad-4096-9819-5a02a943090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # 构建一个从示例到其对应特征的映射。\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # 我们需要填充的字典。\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # 日志记录。\n",
    "    print(f\"正在后处理 {len(examples)} 个示例的预测，这些预测分散在 {len(features)} 个特征中。\")\n",
    "\n",
    "    # 遍历所有示例！\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # 这些是与当前示例关联的特征的索引。\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # 仅在squad_v2为True时使用。\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        # 遍历与当前示例关联的所有特征。\n",
    "        for feature_index in feature_indices:\n",
    "            # 我们获取模型对这个特征的预测。\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # 这将允许我们将logits中的某些位置映射到原始上下文中的文本跨度。\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # 更新最小空预测。\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # 浏览所有的最佳开始和结束logits，为 `n_best_size` 个最佳选择。\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # 不考虑超出范围的答案，原因是索引超出范围或对应于输入ID的部分不在上下文中。\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # 不考虑长度小于0或大于max_answer_length的答案。\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # 在极少数情况下我们没有一个非空预测，我们创建一个假预测以避免失败。\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # 选择我们的最终答案：最佳答案或空答案（仅适用于squad_v2）\n",
    "        if not squad_v2:\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        else:\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example[\"id\"]] = answer\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b958fb40-3496-4600-9dbd-ff8eb2faa477",
   "metadata": {},
   "source": [
    "在原始结果上应用后处理问答结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4940d8a9-aaca-455b-af8a-d99c3f7c61a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在后处理 200 个示例的预测，这些预测分散在 204 个特征中。\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebc0470d6134af3907e8c3bd035cbdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_predictions = postprocess_qa_predictions(small_validation_dataset_200, validation_features, raw_predictions.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26c8776-09eb-4277-821d-5dd03f977436",
   "metadata": {},
   "source": [
    "加载 `SQuAD v2` 的评估指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8f402226-2367-4475-aea1-f48ab2fe48a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datasets import load_metric\n",
    "#metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "84879fff-5766-486c-af6b-a4882d8cc64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"/data/models/evaluate-0.4.5/metrics/squad_v2\" if squad_v2 else \"/data/models/evaluate-0.4.5/metrics/squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d401ae2-659d-4dce-8306-d1e551c1206c",
   "metadata": {},
   "source": [
    "接下来，我们可以调用上面定义的函数进行评估。\n",
    "\n",
    "只需稍微调整一下预测和标签的格式，因为它期望的是一系列字典而不是一个大字典。\n",
    "\n",
    "在使用`squad_v2`数据集时，我们还需要设置`no_answer_probability`参数（我们在这里将其设置为0.0，因为如果我们选择了答案，我们已经将答案设置为空）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "52a1e3a0-a96e-472e-8654-0af868c0a819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 1.0,\n",
       " 'f1': 4.616015014636297,\n",
       " 'total': 200,\n",
       " 'HasAns_exact': 0.0,\n",
       " 'HasAns_f1': 7.69364896731127,\n",
       " 'HasAns_total': 94,\n",
       " 'NoAns_exact': 1.8867924528301887,\n",
       " 'NoAns_f1': 1.8867924528301887,\n",
       " 'NoAns_total': 106,\n",
       " 'best_exact': 53.0,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 53.07952551834131,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if squad_v2:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n",
    "else:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in small_validation_dataset_200]\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f14f11-dbea-4e25-aa9b-4915ba9b1361",
   "metadata": {},
   "source": [
    "### Homework：加载本地保存的模型，进行评估和再训练更高的 F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241aeeee-7402-441b-a036-e7a43891d28f",
   "metadata": {},
   "source": [
    "#### **换更大一些的训练集**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "036f7495-15a3-44ac-b40b-cfdb4332971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "39f603b0-b2e6-4f2f-9998-dba0955cff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset_800 = tokenized_datasets[\"train\"].shuffle(seed=66).select(range(800))\n",
    "small_eval_dataset_800 = tokenized_datasets[\"validation\"].shuffle(seed=66).select(range(800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d201dd4c-034c-4a10-97ff-70ca822985cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ThinkPad\\AppData\\Local\\Temp\\ipykernel_19252\\2344870709.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trained_trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trained_trainer = Trainer(\n",
    "    trained_model,\n",
    "    args,\n",
    "    train_dataset=small_train_dataset_800,\n",
    "    eval_dataset=small_eval_dataset_800,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "98fa2ab1-b4b5-4295-b448-519d76891eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 2:26:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.235181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.458299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.115832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=39, training_loss=5.003631983047876, metrics={'train_runtime': 8894.3812, 'train_samples_per_second': 0.27, 'train_steps_per_second': 0.004, 'total_flos': 235175580057600.0, 'train_loss': 5.003631983047876, 'epoch': 3.0})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "076ee816-8612-4506-b1cc-df44259d6f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir2 = f\"/data/models/distilbert--distilbert-base-uncased-finetuned-squad-800\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b7a067d2-0e84-4c19-a953-7afd188e7114",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = trained_trainer.save_model(model_dir2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dbb6bc88-9315-447d-a76e-9f20465da7a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_predictions = trained_trainer.predict(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "62769741-fb87-4c71-aadf-cfc8c4041ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在后处理 200 个示例的预测，这些预测分散在 204 个特征中。\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4bd8a6335184c5cbcd9f05ea0025d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_predictions = postprocess_qa_predictions(small_validation_dataset_200, validation_features, raw_predictions.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "30888222-aa73-462e-a2b3-ef20017778c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 53.0,\n",
       " 'f1': 53.0,\n",
       " 'total': 200,\n",
       " 'HasAns_exact': 0.0,\n",
       " 'HasAns_f1': 0.0,\n",
       " 'HasAns_total': 94,\n",
       " 'NoAns_exact': 100.0,\n",
       " 'NoAns_f1': 100.0,\n",
       " 'NoAns_total': 106,\n",
       " 'best_exact': 53.0,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 53.0,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if squad_v2:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n",
    "else:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in small_validation_dataset_200]\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9648a526-af8a-43d8-bbe7-fad1e1682b7a",
   "metadata": {},
   "source": [
    "*以上，更大的训练集可以显著提升F1 Score*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab3be55-5643-4bd7-8fc1-2d5ad6376aa7",
   "metadata": {},
   "source": [
    "#### **在小规模训练集上，增加epochs的次数**\n",
    "\n",
    "因使用相同的训练集，看起来F1 Score是一样的，但是Training Loss和Validation Loss明显减小了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3783eca9-2d13-4824-ba3c-33a3ef9239ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "args800 = TrainingArguments(\n",
    "    output_dir=model_dir2,\n",
    "    eval_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_steps=30,\n",
    "    save_total_limit=5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ade0a762-b2ac-4704-8f08-aaa8797d5bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_800 = AutoModelForQuestionAnswering.from_pretrained(model_dir2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "23d0c898-9a1a-4b47-ad2d-ee6c1ead4dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ThinkPad\\AppData\\Local\\Temp\\ipykernel_19252\\4005132131.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trained_trainer_800 = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trained_trainer_800 = Trainer(\n",
    "    trained_model_800,\n",
    "    args800,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d5593bc5-3687-4721-8cc4-eb8b158e09a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [95/95 58:36, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.755088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.506300</td>\n",
       "      <td>2.698053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.506300</td>\n",
       "      <td>2.680476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.886900</td>\n",
       "      <td>2.751134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.489000</td>\n",
       "      <td>2.756043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=95, training_loss=2.938158336438631, metrics={'train_runtime': 3540.9303, 'train_samples_per_second': 0.424, 'train_steps_per_second': 0.027, 'total_flos': 146984737536000.0, 'train_loss': 2.938158336438631, 'epoch': 5.0})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_trainer_800.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "23e2db4c-0dac-48b4-bf16-6170fb00fe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir3 = f\"/data/models/distilbert--distilbert-base-uncased-finetuned-squad-800-100\"\n",
    "model_to_save_3 = trained_trainer_800.save_model(model_dir3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4fada896-8b35-4547-905e-7bd6f8b66676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_predictions_800 = trained_trainer_800.predict(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "22fc0e22-eae3-431e-bbed-14b04a717c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在后处理 200 个示例的预测，这些预测分散在 204 个特征中。\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cbcc259dde94e08a7bcadb7ea2d1d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_predictions_800 = postprocess_qa_predictions(small_validation_dataset_200, validation_features, raw_predictions_800.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e29d1193-61eb-42db-9889-b9539d59f9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 53.0,\n",
       " 'f1': 53.0,\n",
       " 'total': 200,\n",
       " 'HasAns_exact': 0.0,\n",
       " 'HasAns_f1': 0.0,\n",
       " 'HasAns_total': 94,\n",
       " 'NoAns_exact': 100.0,\n",
       " 'NoAns_f1': 100.0,\n",
       " 'NoAns_total': 106,\n",
       " 'best_exact': 53.0,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 53.0,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if squad_v2:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions_800.items()]\n",
    "else:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions_800.items()]\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in small_validation_dataset_200]\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd23495-7ed3-44ad-b8ec-300745812bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (peft)",
   "language": "python",
   "name": "peft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26929574-6611-40c1-a9e2-c3edfd24d825",
   "metadata": {},
   "source": [
    "## 生成第一段文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5ed50f-2841-4ba9-8c3d-e1a3412ac980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb215a33-849b-4afe-9997-6c3516cb97e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"D:models/microsoft-Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"D:/models/microsoft-Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "472307a2-3823-4a24-b848-6fdcad9e9e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "303ab501-b5e0-4fb8-9ae1-214801a9266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6242d043-013d-4b10-a2b1-b821f6184320",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}\n",
    "]\n",
    "\n",
    "output = generator(messages)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92056b0-db48-4aa9-8bf3-ea7bc4f860d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"Create a funny joke about chickens.\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids1 = tokenizer(prompt1, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids1 = input_ids1.to(\"cpu\")\n",
    "\n",
    "# Get the output of the model before the lm_head\n",
    "model_output1 = model.model(input_ids1)\n",
    "\n",
    "# Get the output of the lm_head\n",
    "lm_head_output1 = model.lm_head(model_output1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ac1a1e-78e1-4082-872f-8bf173864f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb241c2-0556-4034-a65c-fed64e23fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取每一时间步的最高分数对应的词的索引\n",
    "predicted_token_ids = torch.argmax(lm_head_output1, dim=-1)\n",
    "\n",
    "# 使用 tokenizer 将 token ids 转换为文本\n",
    "# 注意这里的 tokenizer 应该与你之前用于编码的 tokenizer 相同\n",
    "decoded_output = tokenizer.decode(predicted_token_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f780d83-2968-4869-8749-d795ae002d37",
   "metadata": {},
   "source": [
    "## eee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "379aceaf-28d5-41d3-a88b-105337bf5db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The capital of France is\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids = input_ids.to(\"cpu\")\n",
    "\n",
    "# Get the output of the model before the lm_head\n",
    "model_output = model.model(input_ids)\n",
    "\n",
    "# Get the output of the lm_head\n",
    "lm_head_output = model.lm_head(model_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "601dc717-b15e-4e77-8392-e1f5450f2484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_id = lm_head_output[0,-1].argmax(-1)\n",
    "tokenizer.decode(token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95ad6744-69b0-49fe-9585-8b1906eac6c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 3072])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f912d43-73e9-4d78-80ec-be97c54ac016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 32064])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ecbdaa-42ea-4e50-a75e-e9d769a95d88",
   "metadata": {},
   "source": [
    "## 词源和嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e42958-8dfc-4d0a-a812-ca207d77250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened. <|assistant|>\"\n",
    "\n",
    "# 对输入的提示词进行分词\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cup\")\n",
    "# 生成文本\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=20\n",
    ")\n",
    "# 打印输出\n",
    "print(tokenizer.decode(generation_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b604dc-1618-48a5-aa82-47e814333742",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10642259-8b1a-46c4-8c12-c114c10abc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f14897-cbb0-4daf-aa86-f18dc9aa0e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in input_ids[0]:\n",
    "    print(tokenizer.decode(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1410cc-b3b3-45cc-b31e-fac0df8da362",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(3323))\n",
    "print(tokenizer.decode(622))\n",
    "print(tokenizer.decode([3323,622]))\n",
    "print(tokenizer.decode(29901))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cf00ab-b387-4167-8574-c6e90db8763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_list = [\n",
    "    \"102;194;165\",\"252;141;98\",\"141;160;203\",\n",
    "    \"231;138;195\",\"166;216;84\",\"255;217;47\"\n",
    "]\n",
    "def show_tokens(sentence, tokenizer_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    token_ids = tokenizer(sentence).input_ids\n",
    "    for idx, t in enumerate(token_ids):\n",
    "        print(\n",
    "            f\"\\xlb[0;30;48;2;{colors_list[idx % len(colors_list)]}m\" +\n",
    "            tokenizer.decode(t) +\n",
    "            \"\\xlb[0m\",\n",
    "            end=\" \"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dc5fd9-3f4e-42c5-94a2-2da1a27182c1",
   "metadata": {},
   "source": [
    "## 使用语言模型创建与上下文相关的词嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f33b739-5742-4b44-96c6-f20aafcef68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "# 加载语言模型\n",
    "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
    "# 对句子进行分词\n",
    "tokens = tokenizer(\"Hello world\", return_tensors=\"pt\")\n",
    "# 处理词元\n",
    "output = model(**tokens)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d076fd88-48c1-4129-b135-378e23bb7650",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b75b32-acf7-41ea-852c-a12b51f6f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in tokens[\"input_ids\"][0]:\n",
    "    print(tokenizer.decode(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed066e92-b641-4780-9735-2a39dc4c5748",
   "metadata": {},
   "source": [
    "## 文本嵌入（用于句子和整篇文档）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d63b6b-64be-451c-818c-80b4f436a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# 加载模型\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "# 将文本转换为文本嵌入\n",
    "vector = model.encode(\"Best movie ever!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512f52f7-02f1-451f-9b1a-69b47047cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603622c4-8d40-4591-ad0c-01a472faef7d",
   "metadata": {},
   "source": [
    "## LLM之外的词嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d118d3ab-8f4e-4d9f-9956-3d82de2165dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdownloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mapi\u001b[39;00m\n\u001b[32m      2\u001b[39m model = api.load(\u001b[33m\"\u001b[39m\u001b[33mglove-wiki-gigaword-50\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbbe935-b820-4fd4-819b-69ee34e0ad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar([model[\"king\"]], topn=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb32cb1-707f-40d4-9e4a-2db1b3dc4372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd918385-2bc1-4acd-a3e4-c151f9575da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib import request\n",
    "data = request.urlopen(\"https://storage.googleapis.com/maps-premium/dataset/yes_complete/train.txt\")\n",
    "# 解析播放列表数据集文件。跳过前两行，因为它们只包含元数据\n",
    "lines = data.read().decode(\"utf-8\").split(\"\\n\")[2:]\n",
    "# 删除只有一首歌的播放列表\n",
    "playlists = [s.rstrip().split() for s in lines if len(s.split()) > 1]\n",
    "# 加载歌曲元数据\n",
    "songs_file = request.urlopen(\"https://storage.googleapis.com/maps-premium/dataset/yes_complete/song_hash.txt\")\n",
    "songs_file = songs_file.read().decode(\"utf-8\").split(\"\\n\")\n",
    "songs = [s.rstrip().split(\"\\t\") for s in songs_file]\n",
    "songs_df = pd.DataFrame(data=songs, columns=[\"id\",\"title\",\"artist\"])\n",
    "songs_df = songs_df.set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "695d782a-4c57-49dc-9788-ad5678f6bb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playlist #1:\n",
      " ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '2', '42', '43', '44', '45', '46', '47', '48', '20', '49', '8', '50', '51', '52', '53', '54', '55', '56', '57', '25', '58', '59', '60', '61', '62', '3', '63', '64', '65', '66', '46', '47', '67', '2', '48', '68', '69', '70', '57', '50', '71', '72', '53', '73', '25', '74', '59', '20', '46', '75', '76', '77', '59', '20', '43'], \n",
      "\n",
      "Playlist #2:\n",
      " ['78', '79', '80', '3', '62', '81', '14', '82', '48', '83', '84', '17', '85', '86', '87', '88', '74', '89', '90', '91', '4', '73', '62', '92', '17', '53', '59', '93', '94', '51', '50', '27', '95', '48', '96', '97', '98', '99', '100', '57', '101', '102', '25', '103', '3', '104', '105', '106', '107', '47', '108', '109', '110', '111', '112', '113', '25', '63', '62', '114', '115', '84', '116', '117', '118', '119', '120', '121', '122', '123', '50', '70', '71', '124', '17', '85', '14', '82', '48', '125', '47', '46', '72', '53', '25', '73', '4', '126', '59', '74', '20', '43', '127', '128', '129', '13', '82', '48', '130', '131', '132', '133', '134', '135', '136', '137', '59', '46', '138', '43', '20', '139', '140', '73', '57', '70', '141', '3', '1', '74', '142', '143', '144', '145', '48', '13', '25', '146', '50', '147', '126', '59', '20', '148', '149', '150', '151', '152', '56', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '60', '176', '51', '177', '178', '179', '180', '181', '182', '183', '184', '185', '57', '186', '187', '188', '189', '190', '191', '46', '192', '193', '194', '195', '196', '197', '198', '25', '199', '200', '49', '201', '100', '202', '203', '204', '205', '206', '207', '32', '208', '209', '210']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Playlist #1:\\n {playlists[0]}, \\n\")\n",
    "print(f\"Playlist #2:\\n {playlists[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7160d3f9-9d23-4c7b-ba0c-7ac090967e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# 训练我们的word2vec模型\n",
    "v_model = Word2Vec(playlists, vector_size=32, window=20, negative=50, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f8dc6c9-0fe7-4b45-a283-2d44505c307d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1922', 0.9977813959121704),\n",
       " ('2014', 0.9971864819526672),\n",
       " ('11473', 0.996427059173584),\n",
       " ('2849', 0.99642413854599),\n",
       " ('5586', 0.9960500001907349),\n",
       " ('5634', 0.9957127571105957),\n",
       " ('3116', 0.9955424666404724),\n",
       " ('10084', 0.9953478574752808),\n",
       " ('2640', 0.9949705600738525),\n",
       " ('2104', 0.9948686361312866)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_id = 2172\n",
    "# 让模型找出与歌曲2172相似的歌曲\n",
    "v_model.wv.most_similar(positive=str(song_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c54c35c-ee58-4c83-9523-aa9f1ac05bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title     Fade To Black\n",
      "artist        Metallica\n",
      "Name: 2172 , dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(songs_df.iloc[2172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13233ca0-9ce9-478a-9f42-a212cbe550fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>One</td>\n",
       "      <td>Metallica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>Youth Gone Wild</td>\n",
       "      <td>Skid Row</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11473</th>\n",
       "      <td>Little Guitars</td>\n",
       "      <td>Van Halen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2849</th>\n",
       "      <td>Run To The Hills</td>\n",
       "      <td>Iron Maiden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5586</th>\n",
       "      <td>The Last In Line</td>\n",
       "      <td>Dio</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title       artist\n",
       "id                                   \n",
       "1922                 One    Metallica\n",
       "2014     Youth Gone Wild     Skid Row\n",
       "11473     Little Guitars    Van Halen\n",
       "2849    Run To The Hills  Iron Maiden\n",
       "5586    The Last In Line          Dio"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def print_recommendations(song_id):\n",
    "    similar_songs = np.array(\n",
    "        v_model.wv.most_similar(positive=str(song_id), topn=5)\n",
    "    )[:,0]\n",
    "    return songs_df.iloc[similar_songs]\n",
    "\n",
    "print_recommendations(2172)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e5a5f0-5e9c-413b-8775-43b13425f29a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (peft)",
   "language": "python",
   "name": "peft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
